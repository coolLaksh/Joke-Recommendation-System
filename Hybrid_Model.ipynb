{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import html2text\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import seaborn\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import ast\n",
    "from operator import itemgetter\n",
    "from gensim.models import LdaModel\n",
    "from scipy.sparse.linalg import svds\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.nmf import Nmf\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.decomposition import NMF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach: \n",
    "\n",
    "In this recommendation system approach, we employ a hybrid model that combines collaborative filtering (CF) and content-based filtering (CBF) to enhance the accuracy and explainability of our predictions. We start by addressing the sparsity issue in the user-item matrix through collaborative filtering, identifying similar users based on historical ratings to fill in missing values. Subsequently, we incorporate content-based filtering by extracting features from jokes, such as keywords or genres, to capture the essence of each joke. During testing, we calculate the similarity between target jokes and others using a similarity measure like cosine similarity. We then select a set of most similar jokes based on content features and provide a prediction for the target joke's rating, determined by the average or weighted average rating of the selected similar jokes. This hybrid approach leverages both user preferences and content characteristics, offering a well-rounded recommendation system that effectively addresses the cold start problem and provides clear explanations for its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = pd.read_excel(\"jester-data-1.xls\")\n",
    "dataset2 = pd.read_excel(\"jester-data-2.xls\")\n",
    "dataset3 = pd.read_excel(\"jester-data-3.xls\")\n",
    "\n",
    "def insert_return(frame):\n",
    "    ret_lst = []\n",
    "    for index,row in frame.iterrows():\n",
    "        ret_lst.append(list(row))\n",
    "    \n",
    "    return ret_lst\n",
    "\n",
    "def combine_dataframe(frame1, frame2, frame3):\n",
    "    joke_lst = [\"Number of jokes rated\"]\n",
    "    for i in range(100):\n",
    "        joke_lst.append(f\"joke-{i}\")\n",
    "\n",
    "    rating_lst = []\n",
    "\n",
    "    rating_lst.extend(insert_return(frame1))\n",
    "    rating_lst.extend(insert_return(frame2))\n",
    "    rating_lst.extend(insert_return(frame3))\n",
    "\n",
    "    return pd.DataFrame(data=rating_lst,columns=joke_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = combine_dataframe(dataset1, dataset2,dataset3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of jokes rated</th>\n",
       "      <th>joke-0</th>\n",
       "      <th>joke-1</th>\n",
       "      <th>joke-2</th>\n",
       "      <th>joke-3</th>\n",
       "      <th>joke-4</th>\n",
       "      <th>joke-5</th>\n",
       "      <th>joke-6</th>\n",
       "      <th>joke-7</th>\n",
       "      <th>joke-8</th>\n",
       "      <th>...</th>\n",
       "      <th>joke-90</th>\n",
       "      <th>joke-91</th>\n",
       "      <th>joke-92</th>\n",
       "      <th>joke-93</th>\n",
       "      <th>joke-94</th>\n",
       "      <th>joke-95</th>\n",
       "      <th>joke-96</th>\n",
       "      <th>joke-97</th>\n",
       "      <th>joke-98</th>\n",
       "      <th>joke-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>4.08</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>6.36</td>\n",
       "      <td>4.37</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>-9.66</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-5.34</td>\n",
       "      <td>8.88</td>\n",
       "      <td>...</td>\n",
       "      <td>2.82</td>\n",
       "      <td>-4.95</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>7.86</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-2.14</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-4.32</td>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9.27</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.80</td>\n",
       "      <td>8.16</td>\n",
       "      <td>-2.82</td>\n",
       "      <td>6.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.0</td>\n",
       "      <td>8.50</td>\n",
       "      <td>4.61</td>\n",
       "      <td>-4.17</td>\n",
       "      <td>-5.39</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.60</td>\n",
       "      <td>7.04</td>\n",
       "      <td>4.61</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>...</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.58</td>\n",
       "      <td>4.27</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.73</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.11</td>\n",
       "      <td>6.55</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0</td>\n",
       "      <td>-6.17</td>\n",
       "      <td>-3.54</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-8.50</td>\n",
       "      <td>-7.09</td>\n",
       "      <td>-4.32</td>\n",
       "      <td>-8.69</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>-6.65</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.54</td>\n",
       "      <td>-6.89</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>-3.35</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-9.08</td>\n",
       "      <td>-5.05</td>\n",
       "      <td>-3.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of jokes rated  joke-0  joke-1  joke-2  joke-3  joke-4  joke-5  \\\n",
       "0                  100.0    4.08   -0.29    6.36    4.37   -2.38   -9.66   \n",
       "1                   49.0     NaN     NaN     NaN     NaN    9.03    9.27   \n",
       "2                   48.0     NaN    8.35     NaN     NaN    1.80    8.16   \n",
       "3                   91.0    8.50    4.61   -4.17   -5.39    1.36    1.60   \n",
       "4                  100.0   -6.17   -3.54    0.44   -8.50   -7.09   -4.32   \n",
       "\n",
       "   joke-6  joke-7  joke-8  ...  joke-90  joke-91  joke-92  joke-93  joke-94  \\\n",
       "0   -0.73   -5.34    8.88  ...     2.82    -4.95    -0.29     7.86    -0.19   \n",
       "1    9.03    9.27     NaN  ...      NaN      NaN      NaN     9.08      NaN   \n",
       "2   -2.82    6.21     NaN  ...      NaN      NaN      NaN     0.53      NaN   \n",
       "3    7.04    4.61   -0.44  ...     5.19     5.58     4.27     5.19     5.73   \n",
       "4   -8.69   -0.87   -6.65  ...    -3.54    -6.89    -0.68    -2.96    -2.18   \n",
       "\n",
       "   joke-95  joke-96  joke-97  joke-98  joke-99  \n",
       "0    -2.14     3.06     0.34    -4.32     1.07  \n",
       "1      NaN      NaN      NaN      NaN      NaN  \n",
       "2      NaN      NaN      NaN      NaN      NaN  \n",
       "3     1.55     3.11     6.55     1.80     1.60  \n",
       "4    -3.35     0.05    -9.08    -5.05    -3.45  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.replace(99.0, np.nan, inplace=True)\n",
    "ratings.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 15.098547712450365\n"
     ]
    }
   ],
   "source": [
    "user_item_matrix = ratings.drop(columns=[\"Number of jokes rated\"]).to_numpy()\n",
    "\n",
    "user_item_matrix = np.nan_to_num(user_item_matrix)\n",
    "\n",
    "mask_matrix = np.where(user_item_matrix != 0, 1, 0)\n",
    "\n",
    "U, S, Vt = svds(user_item_matrix, k=8)\n",
    "\n",
    "predicted_matrix = np.dot(np.dot(U, np.diag(S)), Vt)\n",
    "\n",
    "predicted_matrix = np.clip(predicted_matrix, -10, 10)\n",
    "\n",
    "mse = np.sum((predicted_matrix - user_item_matrix)**2) / np.sum(mask_matrix)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_matrix = predicted_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_dataframe = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_joke():\n",
    "\n",
    "    ret_jokes = []\n",
    "\n",
    "    for i in range(1,101):\n",
    "        file = 'init'+str(i)+'.html'\n",
    "        data = codecs.open('jokes/' + file, 'r', encoding=\"cp1252\")\n",
    "        joke_html = data.read()\n",
    "\n",
    "        # Extracting joke\n",
    "        joke = html2text.html2text(joke_html)\n",
    "        # Extracting joke_id\n",
    "        joke_id = int(file.split('init')[1].split('.html')[0])\n",
    "        cleaned_string = re.sub(r'[\\|]+|[-]+', '', joke)\n",
    "\n",
    "        ret_jokes.append(cleaned_string.strip())\n",
    "    \n",
    "    return ret_jokes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jokes = load_clean_joke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Jokes = Jokes[90:]\n",
    "Jokes = Jokes[:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_split1 = CF_matrix[:, :90]\n",
    "matrix_split2 = CF_matrix[:, 90:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_dataframe[\"Original Jokes\"] = Jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_tags_puntuatuions_tags(Joke):\n",
    "    special_char_patterns = r'[^a-zA-Z0-9\\s]'\n",
    "    urlPatterns = r'http\\S+|www\\S+'\n",
    "\n",
    "    text = Joke\n",
    "\n",
    "    text = re.sub(special_char_patterns, ' ', text)\n",
    "    text = re.sub(urlPatterns, ' ', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_stop_words_Tokenization(Joke):\n",
    "    \n",
    "    stopwords_ = set(stopwords.words('english'))\n",
    "    Tokenize_Joke = []\n",
    "    for word in Joke.split():\n",
    "        if word not in stopwords_:\n",
    "            Tokenize_Joke.append(word)\n",
    "\n",
    "    return Tokenize_Joke\n",
    "\n",
    "def lower(Joke):\n",
    "    ret_lst = [word.lower() for word in Joke]\n",
    "    return ret_lst\n",
    "\n",
    "def preprocess_clean_jokes(uncleaned_Jokes):\n",
    "\n",
    "    Cleaned_Jokes = []\n",
    "\n",
    "    for Joke in uncleaned_Jokes:\n",
    "        Joke = remove_tags_puntuatuions_tags(Joke)\n",
    "        Joke = TextBlob(Joke)\n",
    "        Joke = remove_stop_words_Tokenization(Joke)\n",
    "\n",
    "        Cleaned_Jokes.append(lower(Joke))\n",
    "\n",
    "    return Cleaned_Jokes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_jokes = preprocess_clean_jokes(Jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_dataframe[\"PreProcessed Jokes\"] = preprocessed_jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem_tokens(tokenized_text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = []\n",
    "    for preprocessed_joke in tokenized_text:\n",
    "        stemmed_tokens.append([stemmer.stem(token) for token in preprocessed_joke])\n",
    "    \n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "    lemmatized_text = \" \".join([lemmatizer.lemmatize(word) for word in text])\n",
    "    \n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmitized_jokes = [lemmatize_text(text) for text in preprocessed_jokes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_dataframe[\"lemmitized_jokes\"] = lemmitized_jokes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(lemmitized_jokes)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_array = tfidf_matrix.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_lemma = tfidf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_dataframe[\"TF_IDF_Lemma\"] = [i for i in TF_IDF_lemma] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_list = list(jokes_dataframe[\"PreProcessed Jokes\"])\n",
    "\n",
    "merged_lst = []\n",
    "\n",
    "for joke in cleaned_list:\n",
    "    merged_lst.append(\" \".join(joke))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_dataframe[\"merged cleaned\"] = merged_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_dataframe[\"TF_IDF_Lemma\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Jokes</th>\n",
       "      <th>PreProcessed Jokes</th>\n",
       "      <th>lemmitized_jokes</th>\n",
       "      <th>TF_IDF_Lemma</th>\n",
       "      <th>merged cleaned</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PRT</th>\n",
       "      <th>PRON</th>\n",
       "      <th>VERB</th>\n",
       "      <th>PUNC</th>\n",
       "      <th>OTHERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A man visits the doctor. The doctor says \"I ha...</td>\n",
       "      <td>[a, man, visits, doctor, the, doctor, says, i,...</td>\n",
       "      <td>a man visit doctor the doctor say i bad news y...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>a man visits doctor the doctor says i bad news...</td>\n",
       "      <td>[(a, DET), (man, NOUN), (visits, VERB), (docto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This couple had an excellent relationship goin...</td>\n",
       "      <td>[this, couple, excellent, relationship, going,...</td>\n",
       "      <td>this couple excellent relationship going one d...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>this couple excellent relationship going one d...</td>\n",
       "      <td>[(this, DET), (couple, ADJ), (excellent, NOUN)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Original Jokes  \\\n",
       "0  A man visits the doctor. The doctor says \"I ha...   \n",
       "1  This couple had an excellent relationship goin...   \n",
       "\n",
       "                                  PreProcessed Jokes  \\\n",
       "0  [a, man, visits, doctor, the, doctor, says, i,...   \n",
       "1  [this, couple, excellent, relationship, going,...   \n",
       "\n",
       "                                    lemmitized_jokes  \\\n",
       "0  a man visit doctor the doctor say i bad news y...   \n",
       "1  this couple excellent relationship going one d...   \n",
       "\n",
       "                                        TF_IDF_Lemma  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                      merged cleaned  \\\n",
       "0  a man visits doctor the doctor says i bad news...   \n",
       "1  this couple excellent relationship going one d...   \n",
       "\n",
       "                                            pos_tags  ADJ  ADP  ADV CONJ  DET  \\\n",
       "0  [(a, DET), (man, NOUN), (visits, VERB), (docto...  NaN  NaN  NaN  NaN  NaN   \n",
       "1  [(this, DET), (couple, ADJ), (excellent, NOUN)...  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "  NOUN  NUM  PRT PRON VERB PUNC OTHERS  \n",
       "0  NaN  NaN  NaN  NaN  NaN  NaN    NaN  \n",
       "1  NaN  NaN  NaN  NaN  NaN  NaN    NaN  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def posTagging(text):\n",
    "    return nltk.pos_tag(text, tagset='universal')\n",
    "\n",
    "jokes_dataframe['pos_tags'] = jokes_dataframe['PreProcessed Jokes'].apply(lambda x: posTagging(x))\n",
    "jokes_dataframe.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'DET'),\n",
       " ('man', 'NOUN'),\n",
       " ('visits', 'VERB'),\n",
       " ('doctor', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('doctor', 'NOUN'),\n",
       " ('says', 'VERB'),\n",
       " ('i', 'NOUN'),\n",
       " ('bad', 'ADJ'),\n",
       " ('news', 'NOUN'),\n",
       " ('you', 'PRON'),\n",
       " ('cancer', 'NOUN'),\n",
       " ('alzheimer', 'VERB'),\n",
       " ('disease', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('man', 'NOUN'),\n",
       " ('replies', 'VERB'),\n",
       " ('well', 'ADV'),\n",
       " ('thank', 'ADJ'),\n",
       " ('god', 'NOUN'),\n",
       " ('i', 'NOUN'),\n",
       " ('cancer', 'NOUN')]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_dataframe[\"pos_tags\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jokes_dataframe['ADJ'] = pd.Series(dtype=str)\n",
    "# jokes_dataframe['ADP'] = pd.Series(dtype=str)\n",
    "# jokes_dataframe['ADV'] = pd.Series(dtype=str)\n",
    "# jokes_dataframe['CONJ'] = pd.Series(dtype=str)\n",
    "# jokes_dataframe['DET'] = pd.Series(dtype=str)\n",
    "# jokes_dataframe['NOUN'] = pd.Series(dtype=str)\n",
    "# jokes_dataframe['NUM'] = pd.Series(dtype=str)\n",
    "# jokes_dataframe['PRT'] = pd.Series(dtype=str)\n",
    "# jokes_dataframe['PRON'] = pd.Series(dtype=str)\n",
    "# jokes_dataframe['PRT'] = pd.Series(dtype=str)\n",
    "# jokes_dataframe['PRON'] = pd.Series(dtype=str)\n",
    "# jokes_dataframe['VERB'] = pd.Series(dtype=str)\n",
    "# jokes_dataframe['PUNC'] = pd.Series(dtype=str)\n",
    "# jokes_dataframe['OTHERS'] = pd.Series(dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_tags(col_tags, tag_columns = {\n",
    "    'ADJ': 'ADJ',\n",
    "    'ADP': 'ADP',\n",
    "    'ADV': 'ADV',\n",
    "    'CONJ': 'CONJ',\n",
    "    'DET': 'DET',\n",
    "    'NOUN': 'NOUN',\n",
    "    'NUM': 'NUM',\n",
    "    'PRT': 'PRT',\n",
    "    'PRON': 'PRON',\n",
    "    'VERB': 'VERB',\n",
    "    '.': '.',\n",
    "    'X': 'X'}):\n",
    "    aggregated_tags = {col: [] for col in tag_columns.values()}\n",
    "    \n",
    "    ret_frame = pd.DataFrame(columns=aggregated_tags)\n",
    "\n",
    "    for i,joke_tags in enumerate(col_tags):\n",
    "        temp_dict = {val:[] for val in aggregated_tags.keys()}\n",
    "        for tag in joke_tags:\n",
    "            temp_dict[tag[1]].append(tag[0])\n",
    "\n",
    "        ret_frame = pd.concat([ret_frame, pd.DataFrame([temp_dict])], ignore_index=True)\n",
    "\n",
    "    return ret_frame\n",
    "\n",
    "aggregate_tags_frame = aggregate_tags(jokes_dataframe[\"pos_tags\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PRT</th>\n",
       "      <th>PRON</th>\n",
       "      <th>VERB</th>\n",
       "      <th>.</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bad, thank]</td>\n",
       "      <td>[disease]</td>\n",
       "      <td>[well]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[a, the, the]</td>\n",
       "      <td>[man, doctor, i, news, cancer, man, god, i, ca...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[you]</td>\n",
       "      <td>[visits, doctor, says, alzheimer, replies]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[couple, girlfriend, awful, big, old]</td>\n",
       "      <td>[that]</td>\n",
       "      <td>[told, possibly, awfully]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[this]</td>\n",
       "      <td>[excellent, relationship, day, home, work, pac...</td>\n",
       "      <td>[one, ten]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[he, what, they, he]</td>\n",
       "      <td>[going, came, find, asked, leaving, heard, cou...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[long, nelson]</td>\n",
       "      <td>[teeth, willie]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[a, the]</td>\n",
       "      <td>[feet, front, row, concert]</td>\n",
       "      <td>[200, 4]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[what]</td>\n",
       "      <td>[q]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>[around]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[a, a]</td>\n",
       "      <td>[difference, man, toilet, follow, use]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[what]</td>\n",
       "      <td>[q, toilet]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[slash]</td>\n",
       "      <td>[o]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[a]</td>\n",
       "      <td>[j, simpson, internet, address, slash, backsla...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[what]</td>\n",
       "      <td>[q]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>[much]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[how]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[a, the]</td>\n",
       "      <td>[neutron, walks, bar, orders, i, neutron, bart...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[drink, owe, asks, charge]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>[routine, physical, good, bad, ok, good, good,...</td>\n",
       "      <td>[that]</td>\n",
       "      <td>[recently, i, first, then]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[a, the, the, the, the, the, the, the]</td>\n",
       "      <td>[man, examination, phone, call, doctor, doctor...</td>\n",
       "      <td>[24]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[completing, receives, says, says, give, says,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>[czechoslovakian, eyesight, worse, see, simple...</td>\n",
       "      <td>[optometrist]</td>\n",
       "      <td>[steadily]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[a, the]</td>\n",
       "      <td>[man, time, doctor, testing, eye, chart, lette...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[felt, growing, felt, go, started, showed, dim...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>[naval, canadian, please, divert, avoid, avoid...</td>\n",
       "      <td>[navy, lighthouse]</td>\n",
       "      <td>[north, south, north]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[a, this, no, this, this]</td>\n",
       "      <td>[radio, conversation, ship, authorities, cours...</td>\n",
       "      <td>[15, 15, three, three, 15, one, five]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[us, your, us, your, your, we, your]</td>\n",
       "      <td>[americans, recommend, divert, americans, say,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>[many]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[how]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[a, that]</td>\n",
       "      <td>[programmers, change, none, hardware, problem]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[q, take, lightbulb]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  ADJ                 ADP  \\\n",
       "0                                        [bad, thank]           [disease]   \n",
       "1               [couple, girlfriend, awful, big, old]              [that]   \n",
       "2                                      [long, nelson]     [teeth, willie]   \n",
       "3                                                  []            [around]   \n",
       "4                                             [slash]                 [o]   \n",
       "..                                                ...                 ...   \n",
       "85                                             [much]                  []   \n",
       "86  [routine, physical, good, bad, ok, good, good,...              [that]   \n",
       "87  [czechoslovakian, eyesight, worse, see, simple...       [optometrist]   \n",
       "88  [naval, canadian, please, divert, avoid, avoid...  [navy, lighthouse]   \n",
       "89                                             [many]                  []   \n",
       "\n",
       "                           ADV CONJ                                     DET  \\\n",
       "0                       [well]   []                           [a, the, the]   \n",
       "1    [told, possibly, awfully]   []                                  [this]   \n",
       "2                           []   []                                [a, the]   \n",
       "3                           []   []                                  [a, a]   \n",
       "4                           []   []                                     [a]   \n",
       "..                         ...  ...                                     ...   \n",
       "85                       [how]   []                                [a, the]   \n",
       "86  [recently, i, first, then]   []  [a, the, the, the, the, the, the, the]   \n",
       "87                  [steadily]   []                                [a, the]   \n",
       "88       [north, south, north]   []               [a, this, no, this, this]   \n",
       "89                       [how]   []                               [a, that]   \n",
       "\n",
       "                                                 NOUN  \\\n",
       "0   [man, doctor, i, news, cancer, man, god, i, ca...   \n",
       "1   [excellent, relationship, day, home, work, pac...   \n",
       "2                         [feet, front, row, concert]   \n",
       "3              [difference, man, toilet, follow, use]   \n",
       "4   [j, simpson, internet, address, slash, backsla...   \n",
       "..                                                ...   \n",
       "85  [neutron, walks, bar, orders, i, neutron, bart...   \n",
       "86  [man, examination, phone, call, doctor, doctor...   \n",
       "87  [man, time, doctor, testing, eye, chart, lette...   \n",
       "88  [radio, conversation, ship, authorities, cours...   \n",
       "89     [programmers, change, none, hardware, problem]   \n",
       "\n",
       "                                      NUM PRT  \\\n",
       "0                                      []  []   \n",
       "1                              [one, ten]  []   \n",
       "2                                [200, 4]  []   \n",
       "3                                      []  []   \n",
       "4                                      []  []   \n",
       "..                                    ...  ..   \n",
       "85                                     []  []   \n",
       "86                                   [24]  []   \n",
       "87                                     []  []   \n",
       "88  [15, 15, three, three, 15, one, five]  []   \n",
       "89                                     []  []   \n",
       "\n",
       "                                    PRON  \\\n",
       "0                                  [you]   \n",
       "1                   [he, what, they, he]   \n",
       "2                                 [what]   \n",
       "3                                 [what]   \n",
       "4                                 [what]   \n",
       "..                                   ...   \n",
       "85                                    []   \n",
       "86                                    []   \n",
       "87                                    []   \n",
       "88  [us, your, us, your, your, we, your]   \n",
       "89                                    []   \n",
       "\n",
       "                                                 VERB   .   X  \n",
       "0          [visits, doctor, says, alzheimer, replies]  []  []  \n",
       "1   [going, came, find, asked, leaving, heard, cou...  []  []  \n",
       "2                                                 [q]  []  []  \n",
       "3                                         [q, toilet]  []  []  \n",
       "4                                                 [q]  []  []  \n",
       "..                                                ...  ..  ..  \n",
       "85                         [drink, owe, asks, charge]  []  []  \n",
       "86  [completing, receives, says, says, give, says,...  []  []  \n",
       "87  [felt, growing, felt, go, started, showed, dim...  []  []  \n",
       "88  [americans, recommend, divert, americans, say,...  []  []  \n",
       "89                               [q, take, lightbulb]  []  []  \n",
       "\n",
       "[90 rows x 12 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_tags_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pos tagging: https://212digital.medium.com/an-introduction-to-part-of-speech-tagging-what-it-is-and-how-you-can-use-it-in-natural-language-9723f4696f78\n",
    "\n",
    "sentiment analysis â€” By identifying words with positive or negative connotations, POS tagging can be used to calculate the overall sentiment of a piece of text.\n",
    "\n",
    "topic identification â€” By looking at which words are most commonly used together, POS tagging can help automatically identify the main topics of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterByPOS(tags):\n",
    "    try:\n",
    "        tags = ast.literal_eval(tags)\n",
    "    except ValueError as e:\n",
    "        print(\"Error during literal_eval:\", e)\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    txt = []\n",
    "    \n",
    "    for word, pos in tags:\n",
    "        if pos in ['ADJ', 'NOUN'] and len(word) > 1:\n",
    "            txt.append(word)\n",
    "            \n",
    "    return ' '.join(txt)\n",
    "\n",
    "helper = [filterByPOS(str(tags)) for tags in jokes_dataframe[\"pos_tags\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying topic modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "docs = [d.split() for d in helper if d]\n",
    "\n",
    "bigram_model = Phrases(docs, min_count=5, threshold=15)\n",
    "trigram_model = Phrases(bigram_model[docs], min_count=5, threshold=15)\n",
    "\n",
    "docs_with_ngrams = trigram_model[bigram_model[docs]]\n",
    "\n",
    "dictionary = Dictionary(docs_with_ngrams)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs_with_ngrams]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dictionary[0]\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=500,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=700,\n",
    "    num_topics=12,\n",
    "    passes=15,\n",
    "    eval_every=None\n",
    "    ,random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(0.030605953, 'water'),\n",
       "   (0.030605953, 'fire'),\n",
       "   (0.023110582, 'sleep'),\n",
       "   (0.023110578, 'room'),\n",
       "   (0.023110578, 'wakes'),\n",
       "   (0.023110576, 'polish'),\n",
       "   (0.015615202, 'mathematician'),\n",
       "   (0.0156152, 'time'),\n",
       "   (0.0156152, 'physicist'),\n",
       "   (0.0156152, 'engineer'),\n",
       "   (0.015615198, 'douses'),\n",
       "   (0.0081197955, 'sees'),\n",
       "   (0.008119795, 'amount'),\n",
       "   (0.008119795, 'optimal'),\n",
       "   (0.008119795, 'minimum'),\n",
       "   (0.008119795, 'trajectory'),\n",
       "   (0.008119795, 'solution'),\n",
       "   (0.008119795, 'looks'),\n",
       "   (0.008119795, 'sleeping'),\n",
       "   (0.008119795, 'fills')],\n",
       "  -2.778047128167708),\n",
       " ([(0.012594368, 'pope'),\n",
       "   (0.012594368, 'original'),\n",
       "   (0.012594368, 'guardsman'),\n",
       "   (0.012594367, 'steps'),\n",
       "   (0.01259436, 'patio'),\n",
       "   (0.012594355, 'difference'),\n",
       "   (0.0065489626, 'read'),\n",
       "   (0.0065489626, 'asks'),\n",
       "   (0.0065489626, 'scream'),\n",
       "   (0.0065489626, 'available'),\n",
       "   (0.0065489626, 'letter'),\n",
       "   (0.0065489626, 'problem'),\n",
       "   (0.006548962, 'angels'),\n",
       "   (0.006548962, 'master'),\n",
       "   (0.006548962, 'eon'),\n",
       "   (0.006548962, 'comfort'),\n",
       "   (0.006548962, 'committee'),\n",
       "   (0.006548962, 'easy'),\n",
       "   (0.006548962, 'chair'),\n",
       "   (0.006548962, 'angel')],\n",
       "  -8.813976001632174),\n",
       " ([(0.033351794, 'house'),\n",
       "   (0.020229716, 'guess'),\n",
       "   (0.020229716, 'lawyer'),\n",
       "   (0.020229716, 'bed'),\n",
       "   (0.020229712, 'bmw'),\n",
       "   (0.013668672, 'time'),\n",
       "   (0.013668672, 'door'),\n",
       "   (0.01366867, 'arm'),\n",
       "   (0.01366867, 'remarried'),\n",
       "   (0.01366867, 'officer'),\n",
       "   (0.01366867, 'replies'),\n",
       "   (0.0071075996, 'clubs'),\n",
       "   (0.0071075996, 'need'),\n",
       "   (0.0071075996, 'brand'),\n",
       "   (0.0071075987, 'companionship'),\n",
       "   (0.0071075987, 'dollars'),\n",
       "   (0.0071075987, 'hit'),\n",
       "   (0.0071075987, 'golf'),\n",
       "   (0.0071075987, 'rid'),\n",
       "   (0.0071075987, 'car')],\n",
       "  -11.521907744128901),\n",
       " ([(0.025761314, 'difference'),\n",
       "   (0.019463824, 'man'),\n",
       "   (0.01946382, 'oh'),\n",
       "   (0.013151182, 'voice'),\n",
       "   (0.013151182, 'chief'),\n",
       "   (0.013151181, 'booms'),\n",
       "   (0.013151181, 'stone'),\n",
       "   (0.013151178, 'job'),\n",
       "   (0.013151178, 'responsible'),\n",
       "   (0.013151178, 'applicant'),\n",
       "   (0.013151176, 'need'),\n",
       "   (0.013151176, 'woman'),\n",
       "   (0.013151167, 'darwin'),\n",
       "   (0.0068385047, 'life'),\n",
       "   (0.0068385047, 'natives'),\n",
       "   (0.0068385047, 'surrounded'),\n",
       "   (0.0068385038, 'sky'),\n",
       "   (0.0068385038, 'lifeless'),\n",
       "   (0.0068385038, 'body'),\n",
       "   (0.0068385038, 'situation')],\n",
       "  -14.58507993497464),\n",
       " ([(0.038421, 'blah_blah'),\n",
       "   (0.024147125, 'many'),\n",
       "   (0.024097057, 'light'),\n",
       "   (0.01940848, 'william'),\n",
       "   (0.014655344, 'news'),\n",
       "   (0.014655342, 'dad'),\n",
       "   (0.014634561, 'screw'),\n",
       "   (0.01463456, 'bulb'),\n",
       "   (0.009902204, 'mother'),\n",
       "   (0.009902203, 'dianne'),\n",
       "   (0.009902203, 'susan'),\n",
       "   (0.009902203, 'much'),\n",
       "   (0.009902202, 'floor'),\n",
       "   (0.009902202, 'time'),\n",
       "   (0.009902202, 'beautiful'),\n",
       "   (0.009902202, 'last'),\n",
       "   (0.009902202, 'years'),\n",
       "   (0.009902201, 'confession'),\n",
       "   (0.009902201, 'clothes'),\n",
       "   (0.009902197, 'night')],\n",
       "  -15.327049204993886),\n",
       " ([(0.021361472, 'room'),\n",
       "   (0.021361468, 'man'),\n",
       "   (0.018718798, 'engineer'),\n",
       "   (0.016076125, 'pass'),\n",
       "   (0.013433452, 'time'),\n",
       "   (0.013433452, 'bill'),\n",
       "   (0.013433452, 'day'),\n",
       "   (0.010790776, 'machine'),\n",
       "   (0.010790776, 'quiet'),\n",
       "   (0.008148099, 'cindy'),\n",
       "   (0.008148099, 'male'),\n",
       "   (0.008148099, 'first'),\n",
       "   (0.008148099, 'students'),\n",
       "   (0.008148099, 'end'),\n",
       "   (0.008148099, 'company'),\n",
       "   (0.008148099, 'measures'),\n",
       "   (0.008148099, 'island'),\n",
       "   (0.008148099, 'peter'),\n",
       "   (0.008148099, 'hillary'),\n",
       "   (0.008148099, 'years')],\n",
       "  -15.672040465539741),\n",
       " ([(0.019878559, 'hillary'),\n",
       "   (0.019878555, 'bill'),\n",
       "   (0.0150103, 'happy'),\n",
       "   (0.015010299, 'room'),\n",
       "   (0.0101420395, 'fortune'),\n",
       "   (0.0101420395, 'teller'),\n",
       "   (0.0101420395, 'company'),\n",
       "   (0.0101420395, 'recent'),\n",
       "   (0.010142039, 'return'),\n",
       "   (0.010142039, 'everyone'),\n",
       "   (0.010142039, 'someone'),\n",
       "   (0.010142039, 'psychiatrist'),\n",
       "   (0.010142039, 'dollar'),\n",
       "   (0.010142039, 'software'),\n",
       "   (0.010142039, 'doctor'),\n",
       "   (0.010142038, 'piece'),\n",
       "   (0.010142036, 'man'),\n",
       "   (0.005273751, 'repute'),\n",
       "   (0.005273751, 'face'),\n",
       "   (0.005273751, 'single')],\n",
       "  -15.844007186988328),\n",
       " ([(0.030655948, 'guy'),\n",
       "   (0.020577233, 'president'),\n",
       "   (0.015537871, 'tie'),\n",
       "   (0.01553787, 'pants'),\n",
       "   (0.010498506, 'bar'),\n",
       "   (0.010498506, 'partner'),\n",
       "   (0.010498506, 'hell'),\n",
       "   (0.010498506, 'maitre'),\n",
       "   (0.010498505, 'start'),\n",
       "   (0.010498505, 'shot'),\n",
       "   (0.010498502, 'chukcha'),\n",
       "   (0.010498502, 'russian'),\n",
       "   (0.010498502, 'guard'),\n",
       "   (0.010498502, 'parliament'),\n",
       "   (0.010498501, 'man'),\n",
       "   (0.010498497, 'get'),\n",
       "   (0.005459113, 'sort'),\n",
       "   (0.005459113, 'sulk'),\n",
       "   (0.005459113, 'returns'),\n",
       "   (0.005459113, 'code')],\n",
       "  -16.33644345637923),\n",
       " ([(0.022262808, 'little'),\n",
       "   (0.022262806, 'wow'),\n",
       "   (0.016810652, 'broom'),\n",
       "   (0.016810652, 'bow'),\n",
       "   (0.016810652, 'woman'),\n",
       "   (0.016810648, 'duck'),\n",
       "   (0.011358494, 'johnny'),\n",
       "   (0.011358493, 'juan'),\n",
       "   (0.011358493, 'telegram'),\n",
       "   (0.011358493, 'dog'),\n",
       "   (0.011358491, 'family'),\n",
       "   (0.011358491, 'amal'),\n",
       "   (0.011358491, 'wishes'),\n",
       "   (0.011358491, 'husband'),\n",
       "   (0.011358491, 'clerk'),\n",
       "   (0.011358491, 'finger'),\n",
       "   (0.01135849, 'groom'),\n",
       "   (0.0113584865, 'neutron'),\n",
       "   (0.0059063053, 'picture'),\n",
       "   (0.0059063053, 'egypt')],\n",
       "  -16.70799315714716),\n",
       " ([(0.02001463, 'woman'),\n",
       "   (0.018463954, 'man'),\n",
       "   (0.016724534, 'course'),\n",
       "   (0.013434434, 'ship'),\n",
       "   (0.013434434, 'degrees'),\n",
       "   (0.013434432, 'graduate'),\n",
       "   (0.013434432, 'call'),\n",
       "   (0.0134344315, 'degree'),\n",
       "   (0.013434428, 'slash'),\n",
       "   (0.013428306, 'person'),\n",
       "   (0.0101443315, 'builder'),\n",
       "   (0.01014433, 'car'),\n",
       "   (0.0068542217, 'wall'),\n",
       "   (0.0068542217, 'road'),\n",
       "   (0.0068542217, 'beer'),\n",
       "   (0.0068542217, 'lad'),\n",
       "   (0.0068542217, 'stone'),\n",
       "   (0.0068542217, 'sips'),\n",
       "   (0.0068542217, 'collision'),\n",
       "   (0.0068542217, 'look')],\n",
       "  -17.10410591060285),\n",
       " ([(0.022065237, 'swartzeneger'),\n",
       "   (0.011473808, 'sylvester'),\n",
       "   (0.011473808, 'case'),\n",
       "   (0.011473808, 'composers'),\n",
       "   (0.011473808, 'mozart'),\n",
       "   (0.0114738075, 'great'),\n",
       "   (0.0114738075, 'movie'),\n",
       "   (0.0114738075, 'stallone'),\n",
       "   (0.011473801, 'monica'),\n",
       "   (0.011473801, 'dole'),\n",
       "   (0.011473801, 'common'),\n",
       "   (0.011473799, 'bob'),\n",
       "   (0.011473798, 'bill'),\n",
       "   (0.011473795, 'hot'),\n",
       "   (0.0114737945, 'vendor'),\n",
       "   (0.0114737945, 'everything'),\n",
       "   (0.011473794, 'dog'),\n",
       "   (0.011473764, 'shredded'),\n",
       "   (0.011473762, 'tweet'),\n",
       "   (0.0008826162, 'difference')],\n",
       "  -17.595798962828063),\n",
       " ([(0.050225753, 'man'),\n",
       "   (0.029943451, 'news'),\n",
       "   (0.02254127, 'engineer'),\n",
       "   (0.018835833, 'mechanical'),\n",
       "   (0.018825596, 'doctor'),\n",
       "   (0.015130394, 'electrical'),\n",
       "   (0.015130392, 'good'),\n",
       "   (0.015130391, 'engineers'),\n",
       "   (0.015118825, 'bad'),\n",
       "   (0.011424954, 'balloon'),\n",
       "   (0.011424953, 'asian'),\n",
       "   (0.011424953, 'look'),\n",
       "   (0.011424951, 'teller'),\n",
       "   (0.011424951, 'tell'),\n",
       "   (0.011424949, 'school'),\n",
       "   (0.011424948, 'difference'),\n",
       "   (0.011424948, 'civil'),\n",
       "   (0.007719508, 'replies'),\n",
       "   (0.007719508, 'hot'),\n",
       "   (0.007719508, 'balloonist')],\n",
       "  -18.495063020776225)]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus)\n",
    "top_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above code extracts the top topics from the trained LDA model using the top_topics method. Each topic is \n",
    "represented as a list of tuples, where each tuple contains a word and its associated weight in the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_proportions_matrix = np.zeros((len(corpus), 12)) \n",
    "\n",
    "for i, doc_bow in enumerate(corpus):\n",
    "    topic_distribution = model[doc_bow]\n",
    "    \n",
    "    for topic, proportion in topic_distribution:\n",
    "        topic_proportions_matrix[i, topic] = proportion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_dists = model.get_topics()\n",
    "doc_topic_dists = model.get_document_topics(corpus)\n",
    "\n",
    "most_dominant_topics = [max(doc, key=lambda x: x[1])[0] for doc in doc_topic_dists]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features extraction for applying algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def text_analysis(text):\n",
    "\n",
    "    # blob = TextBlob(text)\n",
    "\n",
    "    # sentiment_polarity = blob.sentiment.polarity\n",
    "    # sentiment_subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "\n",
    "    text_length = len(text)\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    punctuation_count = sum(1 for char in words if char in punctuation)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "    word_count = len(words)\n",
    "\n",
    "    unique_words = len(set(words))\n",
    "\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    verb_count = sum(1 for word, pos in pos_tags if pos.startswith('VB'))\n",
    "    noun_count = sum(1 for word, pos in pos_tags if pos.startswith('NN'))\n",
    "\n",
    "    return {\n",
    "        'text_length': text_length,\n",
    "        'punctuation_count': punctuation_count,\n",
    "        'word_count': word_count,\n",
    "        'unique_words': unique_words,\n",
    "        'verb_count': verb_count,\n",
    "        'noun_count': noun_count,\n",
    "        'joke_sentiment_neg': sentiment_scores['neg'],\n",
    "        'joke_sentiment_neu': sentiment_scores['neu'],\n",
    "        'joke_sentiment_pos': sentiment_scores['pos']\n",
    "        }\n",
    "\n",
    "def extract_Featrues(raw_jokes, topic_proportion, num_topics, TF_Vector):\n",
    "\n",
    "    columns=['text_length', 'punctuation_count', 'word_count',\n",
    "                                      'unique_words','verb_count','noun_count', 'joke_sentiment_neg', \n",
    "                                       'joke_sentiment_neu', 'joke_sentiment_pos'\n",
    "                                    ]\n",
    "\n",
    "    lst = [f'topic-{i+1}' for i in range(num_topics)]\n",
    "\n",
    "    columns.extend(lst)\n",
    "    ret_frame = pd.DataFrame(columns=columns)\n",
    "    \n",
    "\n",
    "    for id,joke in enumerate(raw_jokes):\n",
    "        joke = text_analysis(joke)\n",
    "\n",
    "        for topic,proportion in zip(lst, topic_proportion[id]):\n",
    "            joke[topic] = proportion\n",
    "\n",
    "        ret_frame = pd.concat([ret_frame, pd.DataFrame([joke])], ignore_index=True)\n",
    "\n",
    "    return ret_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_jokes = extract_Featrues(list(jokes_dataframe[\"Original Jokes\"]), topic_proportions_matrix, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     A man visits the doctor. The doctor says \"I ha...\n",
       "1     This couple had an excellent relationship goin...\n",
       "2     Q. What's 200 feet long and has 4 teeth?\\n\\nA....\n",
       "3     Q. What's the difference between a man and a t...\n",
       "4     Q. What's O. J. Simpson's Internet address?\\n\\...\n",
       "                            ...                        \n",
       "85    A neutron walks into a bar and orders a drink....\n",
       "86    A man, recently completing a routine physical ...\n",
       "87    A Czechoslovakian man felt his eyesight was gr...\n",
       "88    _A radio conversation of a US naval ship with ...\n",
       "89    Q: How many programmers does it take to change...\n",
       "Name: Original Jokes, Length: 90, dtype: object"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_dataframe[\"Original Jokes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_jokes['joke_id'] = [i for i in range(1,91)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_length</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>joke_sentiment_neg</th>\n",
       "      <th>joke_sentiment_neu</th>\n",
       "      <th>joke_sentiment_pos</th>\n",
       "      <th>topic-1</th>\n",
       "      <th>...</th>\n",
       "      <th>topic-4</th>\n",
       "      <th>topic-5</th>\n",
       "      <th>topic-6</th>\n",
       "      <th>topic-7</th>\n",
       "      <th>topic-8</th>\n",
       "      <th>topic-9</th>\n",
       "      <th>topic-10</th>\n",
       "      <th>topic-11</th>\n",
       "      <th>topic-12</th>\n",
       "      <th>joke_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>162</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.478537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>378</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.974758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.923837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.957356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>137</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.950732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>413</td>\n",
       "      <td>17</td>\n",
       "      <td>42</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.986567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>340</td>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.97859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>790</td>\n",
       "      <td>25</td>\n",
       "      <td>71</td>\n",
       "      <td>47</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.991322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>101</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_length punctuation_count word_count unique_words verb_count  \\\n",
       "0          162                 4         16           13          3   \n",
       "1          378                 6         32           31          9   \n",
       "2           86                 4          9            9          0   \n",
       "3          109                 4          8            7          1   \n",
       "4           94                 9         10            7          0   \n",
       "..         ...               ...        ...          ...        ...   \n",
       "85         137                 8         12           11          3   \n",
       "86         413                17         42           22         10   \n",
       "87         340                15         32           29         11   \n",
       "88         790                25         71           47         12   \n",
       "89         101                 5          8            8          1   \n",
       "\n",
       "   noun_count  joke_sentiment_neg  joke_sentiment_neu  joke_sentiment_pos  \\\n",
       "0          10               0.246               0.691               0.063   \n",
       "1          12               0.041               0.909               0.050   \n",
       "2           4               0.000               1.000               0.000   \n",
       "3           6               0.000               1.000               0.000   \n",
       "4          10               0.440               0.471               0.089   \n",
       "..        ...                 ...                 ...                 ...   \n",
       "85          7               0.095               0.905               0.000   \n",
       "86         20               0.120               0.741               0.140   \n",
       "87         12               0.066               0.903               0.030   \n",
       "88         34               0.116               0.773               0.111   \n",
       "89          5               0.000               1.000               0.000   \n",
       "\n",
       "     topic-1  ...  topic-4  topic-5  topic-6   topic-7  topic-8   topic-9  \\\n",
       "0   0.478537  ...      0.0  0.00000      0.0  0.000000      0.0  0.000000   \n",
       "1   0.000000  ...      0.0  0.00000      0.0  0.974758      0.0  0.000000   \n",
       "2   0.000000  ...      0.0  0.00000      0.0  0.000000      0.0  0.000000   \n",
       "3   0.000000  ...      0.0  0.00000      0.0  0.923837      0.0  0.000000   \n",
       "4   0.000000  ...      0.0  0.00000      0.0  0.000000      0.0  0.000000   \n",
       "..       ...  ...      ...      ...      ...       ...      ...       ...   \n",
       "85  0.000000  ...      0.0  0.00000      0.0  0.000000      0.0  0.950732   \n",
       "86  0.986567  ...      0.0  0.00000      0.0  0.000000      0.0  0.000000   \n",
       "87  0.000000  ...      0.0  0.97859      0.0  0.000000      0.0  0.000000   \n",
       "88  0.000000  ...      0.0  0.00000      0.0  0.000000      0.0  0.000000   \n",
       "89  0.000000  ...      0.0  0.00000      0.0  0.000000      0.0  0.000000   \n",
       "\n",
       "    topic-10  topic-11  topic-12  joke_id  \n",
       "0        0.0  0.483437       0.0        1  \n",
       "1        0.0  0.000000       0.0        2  \n",
       "2        0.0  0.937528       0.0        3  \n",
       "3        0.0  0.000000       0.0        4  \n",
       "4        0.0  0.957356       0.0        5  \n",
       "..       ...       ...       ...      ...  \n",
       "85       0.0  0.000000       0.0       86  \n",
       "86       0.0  0.000000       0.0       87  \n",
       "87       0.0  0.000000       0.0       88  \n",
       "88       0.0  0.991322       0.0       89  \n",
       "89       0.0  0.937529       0.0       90  \n",
       "\n",
       "[90 rows x 22 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features_jokes.drop(columns=[\"joke_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector = TF_IDF_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 21)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_array = np.hstack((features_scaled, tf_idf_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def transform_joke(joke):\n",
    "    tokenized_joke = joke.split()\n",
    "\n",
    "    ngram_joke = trigram_model[bigram_model[tokenized_joke]]\n",
    "\n",
    "    joke_bow = dictionary.doc2bow(ngram_joke)\n",
    "\n",
    "    return joke_bow\n",
    "\n",
    "def find_closest_vectors(target_vector, list_of_vectors, top_n=40):\n",
    "    target_vector = target_vector.reshape(1, -1)\n",
    "\n",
    "    similarities = cosine_similarity(target_vector, list_of_vectors)\n",
    "\n",
    "    top_indices = np.argsort(similarities[0])[-top_n:][::-1]\n",
    "\n",
    "    return top_indices, similarities[0][top_indices]\n",
    "\n",
    "def testing(test_jokes, train_jokes_frame, train_scaler, topic_model, num_topics, user_item_imputed):\n",
    "    preprocess_test = preprocess_clean_jokes(test_jokes)\n",
    "\n",
    "    testing_ratings = []\n",
    "\n",
    "    for joke in preprocess_test:\n",
    "        lemma_test = lemmatize_text(joke)\n",
    "        pos_tag_test = posTagging(lemma_test.split())\n",
    "        filter_Pos = filterByPOS(str(pos_tag_test))\n",
    "        bow_joke = transform_joke(filter_Pos)\n",
    "        topic_distribution = list(topic_model.get_document_topics(bow_joke))\n",
    "        features_extracted = text_analysis(' '.join(joke))\n",
    "        features = list(features_extracted.values())\n",
    "        topic_proportions_matrix = np.zeros((1, 12)) \n",
    "\n",
    "        for i, doc_bow in enumerate([bow_joke]):\n",
    "            topic_distribution = topic_model[doc_bow]\n",
    "            for topic, proportion in topic_distribution:\n",
    "                topic_proportions_matrix[i, topic] = proportion\n",
    "\n",
    "        features.extend(topic_proportions_matrix[0])\n",
    "        # scaled_test = train_scaler.transform(np.array(features).reshape(1, -1))\n",
    "        features = np.array(features)\n",
    "        top_i_closest, similarities = find_closest_vectors(features, train_jokes_frame)\n",
    "\n",
    "        # Calculate weighted average using cosine similarities as weights\n",
    "        weighted_average_ratings = np.average(user_item_imputed[:, top_i_closest], axis=1, weights=similarities)\n",
    "\n",
    "        testing_ratings.append(weighted_average_ratings.tolist())\n",
    "\n",
    "    return testing_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = testing(test_Jokes,features_jokes.drop(columns=[\"joke_id\"]), scaler, model,10,matrix_split1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Jokes</th>\n",
       "      <th>PreProcessed Jokes</th>\n",
       "      <th>lemmitized_jokes</th>\n",
       "      <th>TF_IDF_Lemma</th>\n",
       "      <th>merged cleaned</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PRT</th>\n",
       "      <th>PRON</th>\n",
       "      <th>VERB</th>\n",
       "      <th>PUNC</th>\n",
       "      <th>OTHERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A man visits the doctor. The doctor says \"I ha...</td>\n",
       "      <td>[a, man, visits, doctor, the, doctor, says, i,...</td>\n",
       "      <td>a man visit doctor the doctor say i bad news y...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>a man visits doctor the doctor says i bad news...</td>\n",
       "      <td>[(a, DET), (man, NOUN), (visits, VERB), (docto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This couple had an excellent relationship goin...</td>\n",
       "      <td>[this, couple, excellent, relationship, going,...</td>\n",
       "      <td>this couple excellent relationship going one d...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>this couple excellent relationship going one d...</td>\n",
       "      <td>[(this, DET), (couple, ADJ), (excellent, NOUN)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q. What's 200 feet long and has 4 teeth?\\n\\nA....</td>\n",
       "      <td>[q, what, 200, feet, long, 4, teeth, a, the, f...</td>\n",
       "      <td>q what 200 foot long 4 teeth a the front row w...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>q what 200 feet long 4 teeth a the front row w...</td>\n",
       "      <td>[(q, VERB), (what, PRON), (200, NUM), (feet, N...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q. What's the difference between a man and a t...</td>\n",
       "      <td>[q, what, difference, man, toilet, a, a, toile...</td>\n",
       "      <td>q what difference man toilet a a toilet follow...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>q what difference man toilet a a toilet follow...</td>\n",
       "      <td>[(q, VERB), (what, PRON), (difference, NOUN), ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q. What's O. J. Simpson's Internet address?\\n\\...</td>\n",
       "      <td>[q, what, o, j, simpson, internet, address, a,...</td>\n",
       "      <td>q what o j simpson internet address a slash sl...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>q what o j simpson internet address a slash sl...</td>\n",
       "      <td>[(q, VERB), (what, PRON), (o, ADP), (j, NOUN),...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>A neutron walks into a bar and orders a drink....</td>\n",
       "      <td>[a, neutron, walks, bar, orders, drink, how, m...</td>\n",
       "      <td>a neutron walk bar order drink how much i owe ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>a neutron walks bar orders drink how much i ow...</td>\n",
       "      <td>[(a, DET), (neutron, NOUN), (walks, NOUN), (ba...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>A man, recently completing a routine physical ...</td>\n",
       "      <td>[a, man, recently, completing, routine, physic...</td>\n",
       "      <td>a man recently completing routine physical exa...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>a man recently completing routine physical exa...</td>\n",
       "      <td>[(a, DET), (man, NOUN), (recently, ADV), (comp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>A Czechoslovakian man felt his eyesight was gr...</td>\n",
       "      <td>[a, czechoslovakian, man, felt, eyesight, grow...</td>\n",
       "      <td>a czechoslovakian man felt eyesight growing st...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>a czechoslovakian man felt eyesight growing st...</td>\n",
       "      <td>[(a, DET), (czechoslovakian, ADJ), (man, NOUN)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>_A radio conversation of a US naval ship with ...</td>\n",
       "      <td>[a, radio, conversation, us, naval, ship, cana...</td>\n",
       "      <td>a radio conversation u naval ship canadian aut...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>a radio conversation us naval ship canadian au...</td>\n",
       "      <td>[(a, DET), (radio, NOUN), (conversation, NOUN)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Q: How many programmers does it take to change...</td>\n",
       "      <td>[q, how, many, programmers, take, change, ligh...</td>\n",
       "      <td>q how many programmer take change lightbulb a ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>q how many programmers take change lightbulb a...</td>\n",
       "      <td>[(q, VERB), (how, ADV), (many, ADJ), (programm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Original Jokes  \\\n",
       "0   A man visits the doctor. The doctor says \"I ha...   \n",
       "1   This couple had an excellent relationship goin...   \n",
       "2   Q. What's 200 feet long and has 4 teeth?\\n\\nA....   \n",
       "3   Q. What's the difference between a man and a t...   \n",
       "4   Q. What's O. J. Simpson's Internet address?\\n\\...   \n",
       "..                                                ...   \n",
       "85  A neutron walks into a bar and orders a drink....   \n",
       "86  A man, recently completing a routine physical ...   \n",
       "87  A Czechoslovakian man felt his eyesight was gr...   \n",
       "88  _A radio conversation of a US naval ship with ...   \n",
       "89  Q: How many programmers does it take to change...   \n",
       "\n",
       "                                   PreProcessed Jokes  \\\n",
       "0   [a, man, visits, doctor, the, doctor, says, i,...   \n",
       "1   [this, couple, excellent, relationship, going,...   \n",
       "2   [q, what, 200, feet, long, 4, teeth, a, the, f...   \n",
       "3   [q, what, difference, man, toilet, a, a, toile...   \n",
       "4   [q, what, o, j, simpson, internet, address, a,...   \n",
       "..                                                ...   \n",
       "85  [a, neutron, walks, bar, orders, drink, how, m...   \n",
       "86  [a, man, recently, completing, routine, physic...   \n",
       "87  [a, czechoslovakian, man, felt, eyesight, grow...   \n",
       "88  [a, radio, conversation, us, naval, ship, cana...   \n",
       "89  [q, how, many, programmers, take, change, ligh...   \n",
       "\n",
       "                                     lemmitized_jokes  \\\n",
       "0   a man visit doctor the doctor say i bad news y...   \n",
       "1   this couple excellent relationship going one d...   \n",
       "2   q what 200 foot long 4 teeth a the front row w...   \n",
       "3   q what difference man toilet a a toilet follow...   \n",
       "4   q what o j simpson internet address a slash sl...   \n",
       "..                                                ...   \n",
       "85  a neutron walk bar order drink how much i owe ...   \n",
       "86  a man recently completing routine physical exa...   \n",
       "87  a czechoslovakian man felt eyesight growing st...   \n",
       "88  a radio conversation u naval ship canadian aut...   \n",
       "89  q how many programmer take change lightbulb a ...   \n",
       "\n",
       "                                         TF_IDF_Lemma  \\\n",
       "0   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..                                                ...   \n",
       "85  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "86  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "87  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "88  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "89  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                       merged cleaned  \\\n",
       "0   a man visits doctor the doctor says i bad news...   \n",
       "1   this couple excellent relationship going one d...   \n",
       "2   q what 200 feet long 4 teeth a the front row w...   \n",
       "3   q what difference man toilet a a toilet follow...   \n",
       "4   q what o j simpson internet address a slash sl...   \n",
       "..                                                ...   \n",
       "85  a neutron walks bar orders drink how much i ow...   \n",
       "86  a man recently completing routine physical exa...   \n",
       "87  a czechoslovakian man felt eyesight growing st...   \n",
       "88  a radio conversation us naval ship canadian au...   \n",
       "89  q how many programmers take change lightbulb a...   \n",
       "\n",
       "                                             pos_tags  ADJ  ADP  ADV CONJ  \\\n",
       "0   [(a, DET), (man, NOUN), (visits, VERB), (docto...  NaN  NaN  NaN  NaN   \n",
       "1   [(this, DET), (couple, ADJ), (excellent, NOUN)...  NaN  NaN  NaN  NaN   \n",
       "2   [(q, VERB), (what, PRON), (200, NUM), (feet, N...  NaN  NaN  NaN  NaN   \n",
       "3   [(q, VERB), (what, PRON), (difference, NOUN), ...  NaN  NaN  NaN  NaN   \n",
       "4   [(q, VERB), (what, PRON), (o, ADP), (j, NOUN),...  NaN  NaN  NaN  NaN   \n",
       "..                                                ...  ...  ...  ...  ...   \n",
       "85  [(a, DET), (neutron, NOUN), (walks, NOUN), (ba...  NaN  NaN  NaN  NaN   \n",
       "86  [(a, DET), (man, NOUN), (recently, ADV), (comp...  NaN  NaN  NaN  NaN   \n",
       "87  [(a, DET), (czechoslovakian, ADJ), (man, NOUN)...  NaN  NaN  NaN  NaN   \n",
       "88  [(a, DET), (radio, NOUN), (conversation, NOUN)...  NaN  NaN  NaN  NaN   \n",
       "89  [(q, VERB), (how, ADV), (many, ADJ), (programm...  NaN  NaN  NaN  NaN   \n",
       "\n",
       "    DET NOUN  NUM  PRT PRON VERB PUNC OTHERS  \n",
       "0   NaN  NaN  NaN  NaN  NaN  NaN  NaN    NaN  \n",
       "1   NaN  NaN  NaN  NaN  NaN  NaN  NaN    NaN  \n",
       "2   NaN  NaN  NaN  NaN  NaN  NaN  NaN    NaN  \n",
       "3   NaN  NaN  NaN  NaN  NaN  NaN  NaN    NaN  \n",
       "4   NaN  NaN  NaN  NaN  NaN  NaN  NaN    NaN  \n",
       "..  ...  ...  ...  ...  ...  ...  ...    ...  \n",
       "85  NaN  NaN  NaN  NaN  NaN  NaN  NaN    NaN  \n",
       "86  NaN  NaN  NaN  NaN  NaN  NaN  NaN    NaN  \n",
       "87  NaN  NaN  NaN  NaN  NaN  NaN  NaN    NaN  \n",
       "88  NaN  NaN  NaN  NaN  NaN  NaN  NaN    NaN  \n",
       "89  NaN  NaN  NaN  NaN  NaN  NaN  NaN    NaN  \n",
       "\n",
       "[90 rows x 18 columns]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73418, 10)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(predictions).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73418, 10)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_split2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(matrix_split2, np.array(predictions).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.716337910189438"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3.1006921656900333, MAE: 2.4228822061383517, R-squared: -0.3432699075499501\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Transpose predictions to match the shape of matrix_split2\n",
    "array_x = matrix_split2\n",
    "array_y = np.array(predictions).T\n",
    "\n",
    "# Initialize lists to store scores for each column\n",
    "rmse_values = []\n",
    "mse_values = []\n",
    "mae_values = []\n",
    "r_squared_values = []\n",
    "\n",
    "for col_x, col_y in zip(array_x.T, array_y):\n",
    "    common_length = min(len(col_x), len(col_y))\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(col_x[:common_length], col_y[:common_length]))\n",
    "    rmse_values.append(rmse)\n",
    "\n",
    "    mae = mean_absolute_error(col_x[:common_length], col_y[:common_length])\n",
    "    mae_values.append(mae)\n",
    "\n",
    "    r_squared = r2_score(col_x[:common_length], col_y[:common_length])\n",
    "    r_squared_values.append(r_squared)\n",
    "\n",
    "print(f\"RMSE: {np.mean(rmse)}, MAE: {np.mean(mae)}, R-squared: {np.mean(r_squared)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
