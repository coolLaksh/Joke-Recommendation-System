{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import html2text\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import seaborn\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import ast\n",
    "from operator import itemgetter\n",
    "from gensim.models import LdaModel\n",
    "from scipy.sparse.linalg import svds\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.nmf import Nmf\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.decomposition import NMF\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = pd.read_excel(\"jester-data-1.xls\")\n",
    "dataset2 = pd.read_excel(\"jester-data-2.xls\")\n",
    "dataset3 = pd.read_excel(\"jester-data-3.xls\")\n",
    "\n",
    "def insert_return(frame):\n",
    "    ret_lst = []\n",
    "    for index,row in frame.iterrows():\n",
    "        ret_lst.append(list(row))\n",
    "    \n",
    "    return ret_lst\n",
    "\n",
    "def combine_dataframe(frame1, frame2, frame3):\n",
    "    joke_lst = [\"Number of jokes rated\"]\n",
    "    for i in range(100):\n",
    "        joke_lst.append(f\"joke-{i}\")\n",
    "\n",
    "    rating_lst = []\n",
    "\n",
    "    rating_lst.extend(insert_return(frame1))\n",
    "    rating_lst.extend(insert_return(frame2))\n",
    "    rating_lst.extend(insert_return(frame3))\n",
    "\n",
    "    return pd.DataFrame(data=rating_lst,columns=joke_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = combine_dataframe(dataset1, dataset2,dataset3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of jokes rated</th>\n",
       "      <th>joke-0</th>\n",
       "      <th>joke-1</th>\n",
       "      <th>joke-2</th>\n",
       "      <th>joke-3</th>\n",
       "      <th>joke-4</th>\n",
       "      <th>joke-5</th>\n",
       "      <th>joke-6</th>\n",
       "      <th>joke-7</th>\n",
       "      <th>joke-8</th>\n",
       "      <th>...</th>\n",
       "      <th>joke-90</th>\n",
       "      <th>joke-91</th>\n",
       "      <th>joke-92</th>\n",
       "      <th>joke-93</th>\n",
       "      <th>joke-94</th>\n",
       "      <th>joke-95</th>\n",
       "      <th>joke-96</th>\n",
       "      <th>joke-97</th>\n",
       "      <th>joke-98</th>\n",
       "      <th>joke-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>4.08</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>6.36</td>\n",
       "      <td>4.37</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>-9.66</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-5.34</td>\n",
       "      <td>8.88</td>\n",
       "      <td>...</td>\n",
       "      <td>2.82</td>\n",
       "      <td>-4.95</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>7.86</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-2.14</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-4.32</td>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9.27</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.80</td>\n",
       "      <td>8.16</td>\n",
       "      <td>-2.82</td>\n",
       "      <td>6.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.0</td>\n",
       "      <td>8.50</td>\n",
       "      <td>4.61</td>\n",
       "      <td>-4.17</td>\n",
       "      <td>-5.39</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.60</td>\n",
       "      <td>7.04</td>\n",
       "      <td>4.61</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>...</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.58</td>\n",
       "      <td>4.27</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.73</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.11</td>\n",
       "      <td>6.55</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0</td>\n",
       "      <td>-6.17</td>\n",
       "      <td>-3.54</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-8.50</td>\n",
       "      <td>-7.09</td>\n",
       "      <td>-4.32</td>\n",
       "      <td>-8.69</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>-6.65</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.54</td>\n",
       "      <td>-6.89</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>-3.35</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-9.08</td>\n",
       "      <td>-5.05</td>\n",
       "      <td>-3.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of jokes rated  joke-0  joke-1  joke-2  joke-3  joke-4  joke-5  \\\n",
       "0                  100.0    4.08   -0.29    6.36    4.37   -2.38   -9.66   \n",
       "1                   49.0     NaN     NaN     NaN     NaN    9.03    9.27   \n",
       "2                   48.0     NaN    8.35     NaN     NaN    1.80    8.16   \n",
       "3                   91.0    8.50    4.61   -4.17   -5.39    1.36    1.60   \n",
       "4                  100.0   -6.17   -3.54    0.44   -8.50   -7.09   -4.32   \n",
       "\n",
       "   joke-6  joke-7  joke-8  ...  joke-90  joke-91  joke-92  joke-93  joke-94  \\\n",
       "0   -0.73   -5.34    8.88  ...     2.82    -4.95    -0.29     7.86    -0.19   \n",
       "1    9.03    9.27     NaN  ...      NaN      NaN      NaN     9.08      NaN   \n",
       "2   -2.82    6.21     NaN  ...      NaN      NaN      NaN     0.53      NaN   \n",
       "3    7.04    4.61   -0.44  ...     5.19     5.58     4.27     5.19     5.73   \n",
       "4   -8.69   -0.87   -6.65  ...    -3.54    -6.89    -0.68    -2.96    -2.18   \n",
       "\n",
       "   joke-95  joke-96  joke-97  joke-98  joke-99  \n",
       "0    -2.14     3.06     0.34    -4.32     1.07  \n",
       "1      NaN      NaN      NaN      NaN      NaN  \n",
       "2      NaN      NaN      NaN      NaN      NaN  \n",
       "3     1.55     3.11     6.55     1.80     1.60  \n",
       "4    -3.35     0.05    -9.08    -5.05    -3.45  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratings.replace(99.0, np.nan, inplace=True)\n",
    "ratings.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_dataframe = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_joke():\n",
    "\n",
    "    ret_jokes = []\n",
    "\n",
    "    for i in range(1,101):\n",
    "        file = 'init'+str(i)+'.html'\n",
    "        data = codecs.open('jokes/' + file, 'r', encoding=\"cp1252\")\n",
    "        joke_html = data.read()\n",
    "\n",
    "        # Extracting joke\n",
    "        joke = html2text.html2text(joke_html)\n",
    "        # Extracting joke_id\n",
    "        joke_id = int(file.split('init')[1].split('.html')[0])\n",
    "        cleaned_string = re.sub(r'[\\|]+|[-]+', '', joke)\n",
    "\n",
    "        ret_jokes.append(cleaned_string.strip())\n",
    "    \n",
    "    return ret_jokes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jokes = load_clean_joke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_tags_puntuatuions_tags(Joke):\n",
    "    special_char_patterns = r'[^a-zA-Z0-9\\s]'\n",
    "    urlPatterns = r'http\\S+|www\\S+'\n",
    "\n",
    "    text = Joke\n",
    "\n",
    "    text = re.sub(special_char_patterns, ' ', text)\n",
    "    text = re.sub(urlPatterns, ' ', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_stop_words_Tokenization(Joke):\n",
    "    \n",
    "    stopwords_ = set(stopwords.words('english'))\n",
    "    Tokenize_Joke = []\n",
    "    for word in Joke.split():\n",
    "        if word not in stopwords_:\n",
    "            Tokenize_Joke.append(word)\n",
    "\n",
    "    return Tokenize_Joke\n",
    "\n",
    "def lower(Joke):\n",
    "    ret_lst = [word.lower() for word in Joke]\n",
    "    return ret_lst\n",
    "\n",
    "def preprocess_clean_jokes(uncleaned_Jokes):\n",
    "\n",
    "    Cleaned_Jokes = []\n",
    "\n",
    "    for Joke in uncleaned_Jokes:\n",
    "        Joke = remove_tags_puntuatuions_tags(Joke)\n",
    "        Joke = TextBlob(Joke)\n",
    "        Joke = remove_stop_words_Tokenization(Joke)\n",
    "\n",
    "        Cleaned_Jokes.append(lower(Joke))\n",
    "\n",
    "    return Cleaned_Jokes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_jokes = preprocess_clean_jokes(Jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_dataframe[\"PreProcessed Jokes\"] = preprocessed_jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem_tokens(tokenized_text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = []\n",
    "    for preprocessed_joke in tokenized_text:\n",
    "        stemmed_tokens.append([stemmer.stem(token) for token in preprocessed_joke])\n",
    "    \n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "    lemmatized_text = \" \".join([lemmatizer.lemmatize(word) for word in text])\n",
    "    \n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmitized_jokes = [lemmatize_text(text) for text in preprocessed_jokes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_dataframe[\"lemmitized_jokes\"] = lemmitized_jokes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(lemmitized_jokes)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_lemma = tfidf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_dataframe[\"TF_IDF_Lemma\"] = [i for i in TF_IDF_lemma] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_list = list(jokes_dataframe[\"PreProcessed Jokes\"])\n",
    "\n",
    "merged_lst = []\n",
    "\n",
    "for joke in cleaned_list:\n",
    "    merged_lst.append(\" \".join(joke))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_dataframe[\"merged cleaned\"] = merged_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PreProcessed Jokes</th>\n",
       "      <th>lemmitized_jokes</th>\n",
       "      <th>TF_IDF_Lemma</th>\n",
       "      <th>merged cleaned</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[a, man, visits, doctor, the, doctor, says, i,...</td>\n",
       "      <td>a man visit doctor the doctor say i bad news y...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>a man visits doctor the doctor says i bad news...</td>\n",
       "      <td>[(a, DET), (man, NOUN), (visits, VERB), (docto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[this, couple, excellent, relationship, going,...</td>\n",
       "      <td>this couple excellent relationship going one d...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>this couple excellent relationship going one d...</td>\n",
       "      <td>[(this, DET), (couple, ADJ), (excellent, NOUN)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  PreProcessed Jokes  \\\n",
       "0  [a, man, visits, doctor, the, doctor, says, i,...   \n",
       "1  [this, couple, excellent, relationship, going,...   \n",
       "\n",
       "                                    lemmitized_jokes  \\\n",
       "0  a man visit doctor the doctor say i bad news y...   \n",
       "1  this couple excellent relationship going one d...   \n",
       "\n",
       "                                        TF_IDF_Lemma  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                      merged cleaned  \\\n",
       "0  a man visits doctor the doctor says i bad news...   \n",
       "1  this couple excellent relationship going one d...   \n",
       "\n",
       "                                            pos_tags  \n",
       "0  [(a, DET), (man, NOUN), (visits, VERB), (docto...  \n",
       "1  [(this, DET), (couple, ADJ), (excellent, NOUN)...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def posTagging(text):\n",
    "    return nltk.pos_tag(text, tagset='universal')\n",
    "\n",
    "jokes_dataframe['pos_tags'] = jokes_dataframe['PreProcessed Jokes'].apply(lambda x: posTagging(x))\n",
    "jokes_dataframe.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PreProcessed Jokes</th>\n",
       "      <th>lemmitized_jokes</th>\n",
       "      <th>TF_IDF_Lemma</th>\n",
       "      <th>merged cleaned</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[a, man, visits, doctor, the, doctor, says, i,...</td>\n",
       "      <td>a man visit doctor the doctor say i bad news y...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>a man visits doctor the doctor says i bad news...</td>\n",
       "      <td>[(a, DET), (man, NOUN), (visits, VERB), (docto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[this, couple, excellent, relationship, going,...</td>\n",
       "      <td>this couple excellent relationship going one d...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>this couple excellent relationship going one d...</td>\n",
       "      <td>[(this, DET), (couple, ADJ), (excellent, NOUN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[q, what, 200, feet, long, 4, teeth, a, the, f...</td>\n",
       "      <td>q what 200 foot long 4 teeth a the front row w...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>q what 200 feet long 4 teeth a the front row w...</td>\n",
       "      <td>[(q, VERB), (what, PRON), (200, NUM), (feet, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[q, what, difference, man, toilet, a, a, toile...</td>\n",
       "      <td>q what difference man toilet a a toilet follow...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>q what difference man toilet a a toilet follow...</td>\n",
       "      <td>[(q, VERB), (what, PRON), (difference, NOUN), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[q, what, o, j, simpson, internet, address, a,...</td>\n",
       "      <td>q what o j simpson internet address a slash sl...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>q what o j simpson internet address a slash sl...</td>\n",
       "      <td>[(q, VERB), (what, PRON), (o, ADP), (j, NOUN),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[two, attorneys, went, diner, ordered, two, dr...</td>\n",
       "      <td>two attorney went diner ordered two drink then...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>two attorneys went diner ordered two drinks th...</td>\n",
       "      <td>[(two, NUM), (attorneys, NOUN), (went, VERB), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[a, teacher, explaining, class, different, lan...</td>\n",
       "      <td>a teacher explaining class different language ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>a teacher explaining class different languages...</td>\n",
       "      <td>[(a, DET), (teacher, NOUN), (explaining, VERB)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[age, womanhood, 1, between, ages, 13, 18, she...</td>\n",
       "      <td>age womanhood 1 between age 13 18 she like afr...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>age womanhood 1 between ages 13 18 she like af...</td>\n",
       "      <td>[(age, NOUN), (womanhood, VERB), (1, NUM), (be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[a, bus, station, bus, stops, a, train, statio...</td>\n",
       "      <td>a bus station bus stop a train station train s...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>a bus station bus stops a train station train ...</td>\n",
       "      <td>[(a, DET), (bus, NOUN), (station, NOUN), (bus,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[q, whats, difference, greeting, queen, greeti...</td>\n",
       "      <td>q whats difference greeting queen greeting pre...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>q whats difference greeting queen greeting pre...</td>\n",
       "      <td>[(q, NOUN), (whats, NOUN), (difference, NOUN),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   PreProcessed Jokes  \\\n",
       "0   [a, man, visits, doctor, the, doctor, says, i,...   \n",
       "1   [this, couple, excellent, relationship, going,...   \n",
       "2   [q, what, 200, feet, long, 4, teeth, a, the, f...   \n",
       "3   [q, what, difference, man, toilet, a, a, toile...   \n",
       "4   [q, what, o, j, simpson, internet, address, a,...   \n",
       "..                                                ...   \n",
       "95  [two, attorneys, went, diner, ordered, two, dr...   \n",
       "96  [a, teacher, explaining, class, different, lan...   \n",
       "97  [age, womanhood, 1, between, ages, 13, 18, she...   \n",
       "98  [a, bus, station, bus, stops, a, train, statio...   \n",
       "99  [q, whats, difference, greeting, queen, greeti...   \n",
       "\n",
       "                                     lemmitized_jokes  \\\n",
       "0   a man visit doctor the doctor say i bad news y...   \n",
       "1   this couple excellent relationship going one d...   \n",
       "2   q what 200 foot long 4 teeth a the front row w...   \n",
       "3   q what difference man toilet a a toilet follow...   \n",
       "4   q what o j simpson internet address a slash sl...   \n",
       "..                                                ...   \n",
       "95  two attorney went diner ordered two drink then...   \n",
       "96  a teacher explaining class different language ...   \n",
       "97  age womanhood 1 between age 13 18 she like afr...   \n",
       "98  a bus station bus stop a train station train s...   \n",
       "99  q whats difference greeting queen greeting pre...   \n",
       "\n",
       "                                         TF_IDF_Lemma  \\\n",
       "0   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..                                                ...   \n",
       "95  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "96  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "97  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "98  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "99  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                       merged cleaned  \\\n",
       "0   a man visits doctor the doctor says i bad news...   \n",
       "1   this couple excellent relationship going one d...   \n",
       "2   q what 200 feet long 4 teeth a the front row w...   \n",
       "3   q what difference man toilet a a toilet follow...   \n",
       "4   q what o j simpson internet address a slash sl...   \n",
       "..                                                ...   \n",
       "95  two attorneys went diner ordered two drinks th...   \n",
       "96  a teacher explaining class different languages...   \n",
       "97  age womanhood 1 between ages 13 18 she like af...   \n",
       "98  a bus station bus stops a train station train ...   \n",
       "99  q whats difference greeting queen greeting pre...   \n",
       "\n",
       "                                             pos_tags  \n",
       "0   [(a, DET), (man, NOUN), (visits, VERB), (docto...  \n",
       "1   [(this, DET), (couple, ADJ), (excellent, NOUN)...  \n",
       "2   [(q, VERB), (what, PRON), (200, NUM), (feet, N...  \n",
       "3   [(q, VERB), (what, PRON), (difference, NOUN), ...  \n",
       "4   [(q, VERB), (what, PRON), (o, ADP), (j, NOUN),...  \n",
       "..                                                ...  \n",
       "95  [(two, NUM), (attorneys, NOUN), (went, VERB), ...  \n",
       "96  [(a, DET), (teacher, NOUN), (explaining, VERB)...  \n",
       "97  [(age, NOUN), (womanhood, VERB), (1, NUM), (be...  \n",
       "98  [(a, DET), (bus, NOUN), (station, NOUN), (bus,...  \n",
       "99  [(q, NOUN), (whats, NOUN), (difference, NOUN),...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jokes_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import spacy\n",
    "import textstat\n",
    "\n",
    "def detect_irony(joke):\n",
    "    blob = TextBlob(joke)\n",
    "    \n",
    "    irony_detected = blob.sentiment.polarity < 0\n",
    "    \n",
    "    return irony_detected\n",
    "\n",
    "def extract_humor_features(joke):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    doc = nlp(joke)\n",
    "\n",
    "    wordplay_detected = False\n",
    "    for token in doc:\n",
    "        if token.lemma_.lower() == \"play\" and token.pos_ == \"NOUN\":\n",
    "            wordplay_detected = True\n",
    "            break\n",
    "\n",
    "    incongruity_detected = False\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"prep\" and token.head.pos_ == \"NOUN\":\n",
    "            incongruity_detected = True\n",
    "            break\n",
    "\n",
    "    return wordplay_detected, incongruity_detected\n",
    "\n",
    "def analyze_sentiment(joke):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    scores = sia.polarity_scores(joke)\n",
    "    \n",
    "    compound_score = scores['compound']\n",
    "    sentiment = 'positive' if compound_score > 0 else ('negative' if compound_score < 0 else 'neutral')\n",
    "    \n",
    "    return scores['neg'], scores['pos'], scores['neu']\n",
    "\n",
    "def extract_structure_features(joke):\n",
    "    sentences = sent_tokenize(joke)\n",
    "\n",
    "    sentence_lengths = [len(word_tokenize(sentence)) for sentence in sentences]\n",
    "    avg_sentence_length = sum(sentence_lengths) / len(sentences)\n",
    "    max_sentence_length = max(sentence_lengths)\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+|\\S')\n",
    "    tokens = tokenizer.tokenize(joke)\n",
    "    punctuation_count = len([token for token in tokens if token.isalnum() == False])\n",
    "\n",
    "    paragraphs = joke.split('\\n')\n",
    "    paragraph_count = len(paragraphs)\n",
    "\n",
    "    return avg_sentence_length, max_sentence_length, punctuation_count, paragraph_count\n",
    "\n",
    "def extract_pos_and_grammar_features(joke):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    doc = nlp(joke)\n",
    "\n",
    "    pos_distribution = {token.pos_: pos_count for pos_count, token in enumerate(doc)}\n",
    "\n",
    "    grammar_quality = \"Good\" if not any(token.is_space for token in doc if token.pos_ == \"VERB\") else \"Poor\"\n",
    "\n",
    "    pos_lst = ['SPACE', 'SCONJ', 'AUX', 'DET', 'NOUN', 'VERB', 'PUNCT', 'PRON', 'ADJ', 'ADP']\n",
    "\n",
    "    for pos in pos_lst:\n",
    "        if pos not in pos_distribution.keys():\n",
    "            pos_distribution[pos] = 0\n",
    "\n",
    "    return pos_distribution, grammar_quality\n",
    "\n",
    "def extract_lexical_features(joke):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    doc = nlp(joke)\n",
    "\n",
    "    vocabulary = [token.text.lower() for token in doc if token.is_alpha]\n",
    "    vocabulary_richness = len(set(vocabulary)) / len(vocabulary) if len(vocabulary) > 0 else 0\n",
    "\n",
    "    lexical_diversity = len(set(vocabulary)) / len(doc) if len(doc) > 0 else 0\n",
    "\n",
    "    return vocabulary_richness, lexical_diversity\n",
    "\n",
    "def extract_stylistic_features(joke):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    doc = nlp(joke)\n",
    "\n",
    "    readability_scores = {\n",
    "        'flesch_reading_ease': textstat.flesch_reading_ease(joke),\n",
    "        'flesch_kincaid_grade': textstat.flesch_kincaid_grade(joke),\n",
    "        'automated_readability_index': textstat.automated_readability_index(joke),\n",
    "    }\n",
    "\n",
    "    jargon_slang_detected = any(token.is_alpha and token.text.lower() in [\"jargon\", \"slang\"] for token in doc)\n",
    "\n",
    "    return readability_scores, jargon_slang_detected\n",
    "\n",
    "def feature_Extraction(Org_Jokes, Lemma_Jokes):\n",
    "    \n",
    "    Jokes_Features = []\n",
    "\n",
    "    column_names = ['neg_content', 'pos_content', 'neu_content', 'avg_sentence_length', 'max_sentence_length',\n",
    "                'punctuation_count', 'paragraph_count', 'wordplay_detected', 'incongruity_detected',\n",
    "                'len_filtered_words', 'len_set_filtered_words', 'pos_distribution_VERB', 'pos_distribution_NOUN',\n",
    "                'pos_distribution_ADJ', 'vocabulary_richness', 'lexical_diversity', 'jargon_slang_detected',\n",
    "                'flesch_reading_ease', 'flesch_kincaid_grade', 'automated_readability_index'\n",
    "                , 'irony']\n",
    "\n",
    "    for Org_Joke, Lemma_Joke in zip(Org_Jokes, Lemma_Jokes):\n",
    "        \n",
    "        Org_Joke = ' '.join(Org_Joke)\n",
    "        words = word_tokenize(Org_Joke)\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "        neg_content, pos_content, neu_content = analyze_sentiment(Org_Joke)\n",
    "        avg_sentence_length, max_sentence_length, punctuation_count, paragraph_count = extract_structure_features(Org_Joke)\n",
    "        wordplay_detected, incongruity_detected = extract_humor_features(Org_Joke)\n",
    "        pos_distribution, grammar_quality = extract_pos_and_grammar_features(Lemma_Joke)\n",
    "        vocabulary_richness, lexical_diversity = extract_lexical_features(Lemma_Joke)\n",
    "        readability_scores, jargon_slang_detected = extract_stylistic_features(Org_Joke)\n",
    "        irony =detect_irony(Lemma_Joke)\n",
    "\n",
    "        Jokes_Features.append([neg_content, pos_content, neu_content, avg_sentence_length, max_sentence_length, punctuation_count, paragraph_count\n",
    "    , wordplay_detected, incongruity_detected, len(filtered_words), len(set(filtered_words)), \n",
    "    pos_distribution[\"VERB\"], pos_distribution[\"NOUN\"],  pos_distribution[\"ADJ\"], vocabulary_richness, lexical_diversity,\n",
    "    jargon_slang_detected, readability_scores['flesch_reading_ease'], readability_scores['flesch_kincaid_grade'],\n",
    "    readability_scores['automated_readability_index'],irony\n",
    "    ])\n",
    "\n",
    "    return pd.DataFrame(Jokes_Features, columns=column_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_Jokes = feature_Extraction(list(jokes_dataframe[\"PreProcessed Jokes\"]), list(jokes_dataframe[\"lemmitized_jokes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg_content</th>\n",
       "      <th>pos_content</th>\n",
       "      <th>neu_content</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>max_sentence_length</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>paragraph_count</th>\n",
       "      <th>wordplay_detected</th>\n",
       "      <th>incongruity_detected</th>\n",
       "      <th>len_filtered_words</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_distribution_VERB</th>\n",
       "      <th>pos_distribution_NOUN</th>\n",
       "      <th>pos_distribution_ADJ</th>\n",
       "      <th>vocabulary_richness</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>jargon_slang_detected</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>irony</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.384</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.406</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>False</td>\n",
       "      <td>74.53</td>\n",
       "      <td>8.3</td>\n",
       "      <td>9.7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.070</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.843</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>False</td>\n",
       "      <td>49.83</td>\n",
       "      <td>15.8</td>\n",
       "      <td>21.4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>False</td>\n",
       "      <td>99.57</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>False</td>\n",
       "      <td>68.77</td>\n",
       "      <td>6.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.556</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.331</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>False</td>\n",
       "      <td>74.19</td>\n",
       "      <td>6.4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>False</td>\n",
       "      <td>32.57</td>\n",
       "      <td>16.2</td>\n",
       "      <td>23.1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.278</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.483</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>False</td>\n",
       "      <td>1.44</td>\n",
       "      <td>24.0</td>\n",
       "      <td>29.6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.074</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.599</td>\n",
       "      <td>57.0</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>55</td>\n",
       "      <td>47</td>\n",
       "      <td>34</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.508772</td>\n",
       "      <td>False</td>\n",
       "      <td>22.08</td>\n",
       "      <td>24.3</td>\n",
       "      <td>29.6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.758</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>False</td>\n",
       "      <td>90.09</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.9</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.493</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>False</td>\n",
       "      <td>72.16</td>\n",
       "      <td>7.2</td>\n",
       "      <td>10.7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    neg_content  pos_content  neu_content  avg_sentence_length  \\\n",
       "0         0.384        0.209        0.406                 22.0   \n",
       "1         0.070        0.087        0.843                 38.0   \n",
       "2         0.000        0.000        1.000                 14.0   \n",
       "3         0.000        0.000        1.000                 11.0   \n",
       "4         0.556        0.113        0.331                 14.0   \n",
       "..          ...          ...          ...                  ...   \n",
       "95        0.000        0.000        1.000                 30.0   \n",
       "96        0.278        0.238        0.483                 44.0   \n",
       "97        0.074        0.327        0.599                 57.0   \n",
       "98        0.242        0.000        0.758                 15.0   \n",
       "99        0.000        0.507        0.493                 16.0   \n",
       "\n",
       "    max_sentence_length  punctuation_count  paragraph_count  \\\n",
       "0                    22                  0                1   \n",
       "1                    38                  0                1   \n",
       "2                    14                  0                1   \n",
       "3                    11                  0                1   \n",
       "4                    14                  0                1   \n",
       "..                  ...                ...              ...   \n",
       "95                   30                  0                1   \n",
       "96                   44                  0                1   \n",
       "97                   57                  0                1   \n",
       "98                   15                  0                1   \n",
       "99                   16                  0                1   \n",
       "\n",
       "    wordplay_detected  incongruity_detected  len_filtered_words  ...  \\\n",
       "0               False                 False                  16  ...   \n",
       "1               False                 False                  32  ...   \n",
       "2               False                 False                  11  ...   \n",
       "3               False                 False                   8  ...   \n",
       "4               False                 False                  11  ...   \n",
       "..                ...                   ...                 ...  ...   \n",
       "95              False                 False                  26  ...   \n",
       "96              False                 False                  39  ...   \n",
       "97              False                  True                  48  ...   \n",
       "98              False                  True                  11  ...   \n",
       "99              False                 False                  14  ...   \n",
       "\n",
       "    pos_distribution_VERB  pos_distribution_NOUN  pos_distribution_ADJ  \\\n",
       "0                      18                     21                     8   \n",
       "1                      30                     36                    37   \n",
       "2                      13                     11                     0   \n",
       "3                       4                     10                     0   \n",
       "4                       0                     13                     0   \n",
       "..                    ...                    ...                   ...   \n",
       "95                     28                     29                    26   \n",
       "96                     41                     40                    36   \n",
       "97                     55                     47                    34   \n",
       "98                     13                     14                     0   \n",
       "99                     12                     16                     0   \n",
       "\n",
       "    vocabulary_richness  lexical_diversity  jargon_slang_detected  \\\n",
       "0              0.772727           0.772727                  False   \n",
       "1              0.947368           0.947368                  False   \n",
       "2              1.000000           0.857143                  False   \n",
       "3              0.818182           0.818182                  False   \n",
       "4              0.785714           0.785714                  False   \n",
       "..                  ...                ...                    ...   \n",
       "95             0.800000           0.800000                  False   \n",
       "96             0.568182           0.568182                  False   \n",
       "97             0.674419           0.508772                  False   \n",
       "98             0.600000           0.600000                  False   \n",
       "99             0.882353           0.882353                  False   \n",
       "\n",
       "    flesch_reading_ease  flesch_kincaid_grade  automated_readability_index  \\\n",
       "0                 74.53                   8.3                          9.7   \n",
       "1                 49.83                  15.8                         21.4   \n",
       "2                 99.57                   2.9                          3.4   \n",
       "3                 68.77                   6.4                          4.2   \n",
       "4                 74.19                   6.4                          7.4   \n",
       "..                  ...                   ...                          ...   \n",
       "95                32.57                  16.2                         23.1   \n",
       "96                 1.44                  24.0                         29.6   \n",
       "97                22.08                  24.3                         29.6   \n",
       "98                90.09                   4.4                          4.9   \n",
       "99                72.16                   7.2                         10.7   \n",
       "\n",
       "    irony  \n",
       "0    True  \n",
       "1   False  \n",
       "2    True  \n",
       "3    True  \n",
       "4   False  \n",
       "..    ...  \n",
       "95  False  \n",
       "96   True  \n",
       "97  False  \n",
       "98  False  \n",
       "99  False  \n",
       "\n",
       "[100 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_Jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_Jokes = features_Jokes.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg_content</th>\n",
       "      <th>pos_content</th>\n",
       "      <th>neu_content</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>max_sentence_length</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>paragraph_count</th>\n",
       "      <th>wordplay_detected</th>\n",
       "      <th>incongruity_detected</th>\n",
       "      <th>len_filtered_words</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_distribution_VERB</th>\n",
       "      <th>pos_distribution_NOUN</th>\n",
       "      <th>pos_distribution_ADJ</th>\n",
       "      <th>vocabulary_richness</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>jargon_slang_detected</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>irony</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>55</td>\n",
       "      <td>47</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    neg_content  pos_content  neu_content  avg_sentence_length  \\\n",
       "0             0            0            0                   22   \n",
       "1             0            0            0                   38   \n",
       "2             0            0            1                   14   \n",
       "3             0            0            1                   11   \n",
       "4             0            0            0                   14   \n",
       "..          ...          ...          ...                  ...   \n",
       "95            0            0            1                   30   \n",
       "96            0            0            0                   44   \n",
       "97            0            0            0                   57   \n",
       "98            0            0            0                   15   \n",
       "99            0            0            0                   16   \n",
       "\n",
       "    max_sentence_length  punctuation_count  paragraph_count  \\\n",
       "0                    22                  0                1   \n",
       "1                    38                  0                1   \n",
       "2                    14                  0                1   \n",
       "3                    11                  0                1   \n",
       "4                    14                  0                1   \n",
       "..                  ...                ...              ...   \n",
       "95                   30                  0                1   \n",
       "96                   44                  0                1   \n",
       "97                   57                  0                1   \n",
       "98                   15                  0                1   \n",
       "99                   16                  0                1   \n",
       "\n",
       "    wordplay_detected  incongruity_detected  len_filtered_words  ...  \\\n",
       "0                   0                     0                  16  ...   \n",
       "1                   0                     0                  32  ...   \n",
       "2                   0                     0                  11  ...   \n",
       "3                   0                     0                   8  ...   \n",
       "4                   0                     0                  11  ...   \n",
       "..                ...                   ...                 ...  ...   \n",
       "95                  0                     0                  26  ...   \n",
       "96                  0                     0                  39  ...   \n",
       "97                  0                     1                  48  ...   \n",
       "98                  0                     1                  11  ...   \n",
       "99                  0                     0                  14  ...   \n",
       "\n",
       "    pos_distribution_VERB  pos_distribution_NOUN  pos_distribution_ADJ  \\\n",
       "0                      18                     21                     8   \n",
       "1                      30                     36                    37   \n",
       "2                      13                     11                     0   \n",
       "3                       4                     10                     0   \n",
       "4                       0                     13                     0   \n",
       "..                    ...                    ...                   ...   \n",
       "95                     28                     29                    26   \n",
       "96                     41                     40                    36   \n",
       "97                     55                     47                    34   \n",
       "98                     13                     14                     0   \n",
       "99                     12                     16                     0   \n",
       "\n",
       "    vocabulary_richness  lexical_diversity  jargon_slang_detected  \\\n",
       "0                     0                  0                      0   \n",
       "1                     0                  0                      0   \n",
       "2                     1                  0                      0   \n",
       "3                     0                  0                      0   \n",
       "4                     0                  0                      0   \n",
       "..                  ...                ...                    ...   \n",
       "95                    0                  0                      0   \n",
       "96                    0                  0                      0   \n",
       "97                    0                  0                      0   \n",
       "98                    0                  0                      0   \n",
       "99                    0                  0                      0   \n",
       "\n",
       "    flesch_reading_ease  flesch_kincaid_grade  automated_readability_index  \\\n",
       "0                    74                     8                            9   \n",
       "1                    49                    15                           21   \n",
       "2                    99                     2                            3   \n",
       "3                    68                     6                            4   \n",
       "4                    74                     6                            7   \n",
       "..                  ...                   ...                          ...   \n",
       "95                   32                    16                           23   \n",
       "96                    1                    24                           29   \n",
       "97                   22                    24                           29   \n",
       "98                   90                     4                            4   \n",
       "99                   72                     7                           10   \n",
       "\n",
       "    irony  \n",
       "0       1  \n",
       "1       0  \n",
       "2       1  \n",
       "3       1  \n",
       "4       0  \n",
       "..    ...  \n",
       "95      0  \n",
       "96      1  \n",
       "97      0  \n",
       "98      0  \n",
       "99      0  \n",
       "\n",
       "[100 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_Jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = pd.read_excel(\"jester-data-1.xls\")\n",
    "dataset2 = pd.read_excel(\"jester-data-2.xls\")\n",
    "dataset3 = pd.read_excel(\"jester-data-3.xls\")\n",
    "\n",
    "def insert_return(frame):\n",
    "    ret_lst = []\n",
    "    for index,row in frame.iterrows():\n",
    "        ret_lst.append(list(row))\n",
    "    \n",
    "    return ret_lst\n",
    "\n",
    "def combine_dataframe(frame1, frame2, frame3):\n",
    "    joke_lst = [\"Number of jokes rated\"]\n",
    "    for i in range(100):\n",
    "        joke_lst.append(f\"joke-{i}\")\n",
    "\n",
    "    rating_lst = []\n",
    "\n",
    "    rating_lst.extend(insert_return(frame1))\n",
    "    rating_lst.extend(insert_return(frame2))\n",
    "    rating_lst.extend(insert_return(frame3))\n",
    "\n",
    "    return pd.DataFrame(data=rating_lst,columns=joke_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of jokes rated</th>\n",
       "      <th>joke-0</th>\n",
       "      <th>joke-1</th>\n",
       "      <th>joke-2</th>\n",
       "      <th>joke-3</th>\n",
       "      <th>joke-4</th>\n",
       "      <th>joke-5</th>\n",
       "      <th>joke-6</th>\n",
       "      <th>joke-7</th>\n",
       "      <th>joke-8</th>\n",
       "      <th>...</th>\n",
       "      <th>joke-90</th>\n",
       "      <th>joke-91</th>\n",
       "      <th>joke-92</th>\n",
       "      <th>joke-93</th>\n",
       "      <th>joke-94</th>\n",
       "      <th>joke-95</th>\n",
       "      <th>joke-96</th>\n",
       "      <th>joke-97</th>\n",
       "      <th>joke-98</th>\n",
       "      <th>joke-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>4.08</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>6.36</td>\n",
       "      <td>4.37</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>-9.66</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-5.34</td>\n",
       "      <td>8.88</td>\n",
       "      <td>...</td>\n",
       "      <td>2.82</td>\n",
       "      <td>-4.95</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>7.86</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-2.14</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-4.32</td>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9.27</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.80</td>\n",
       "      <td>8.16</td>\n",
       "      <td>-2.82</td>\n",
       "      <td>6.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.0</td>\n",
       "      <td>8.50</td>\n",
       "      <td>4.61</td>\n",
       "      <td>-4.17</td>\n",
       "      <td>-5.39</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.60</td>\n",
       "      <td>7.04</td>\n",
       "      <td>4.61</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>...</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.58</td>\n",
       "      <td>4.27</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.73</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.11</td>\n",
       "      <td>6.55</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0</td>\n",
       "      <td>-6.17</td>\n",
       "      <td>-3.54</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-8.50</td>\n",
       "      <td>-7.09</td>\n",
       "      <td>-4.32</td>\n",
       "      <td>-8.69</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>-6.65</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.54</td>\n",
       "      <td>-6.89</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>-3.35</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-9.08</td>\n",
       "      <td>-5.05</td>\n",
       "      <td>-3.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of jokes rated  joke-0  joke-1  joke-2  joke-3  joke-4  joke-5  \\\n",
       "0                  100.0    4.08   -0.29    6.36    4.37   -2.38   -9.66   \n",
       "1                   49.0     NaN     NaN     NaN     NaN    9.03    9.27   \n",
       "2                   48.0     NaN    8.35     NaN     NaN    1.80    8.16   \n",
       "3                   91.0    8.50    4.61   -4.17   -5.39    1.36    1.60   \n",
       "4                  100.0   -6.17   -3.54    0.44   -8.50   -7.09   -4.32   \n",
       "\n",
       "   joke-6  joke-7  joke-8  ...  joke-90  joke-91  joke-92  joke-93  joke-94  \\\n",
       "0   -0.73   -5.34    8.88  ...     2.82    -4.95    -0.29     7.86    -0.19   \n",
       "1    9.03    9.27     NaN  ...      NaN      NaN      NaN     9.08      NaN   \n",
       "2   -2.82    6.21     NaN  ...      NaN      NaN      NaN     0.53      NaN   \n",
       "3    7.04    4.61   -0.44  ...     5.19     5.58     4.27     5.19     5.73   \n",
       "4   -8.69   -0.87   -6.65  ...    -3.54    -6.89    -0.68    -2.96    -2.18   \n",
       "\n",
       "   joke-95  joke-96  joke-97  joke-98  joke-99  \n",
       "0    -2.14     3.06     0.34    -4.32     1.07  \n",
       "1      NaN      NaN      NaN      NaN      NaN  \n",
       "2      NaN      NaN      NaN      NaN      NaN  \n",
       "3     1.55     3.11     6.55     1.80     1.60  \n",
       "4    -3.35     0.05    -9.08    -5.05    -3.45  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratings = combine_dataframe(dataset1, dataset2,dataset3)\n",
    "ratings.replace(99.0, np.nan, inplace=True)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_averages = ratings.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_jokes = list(column_averages)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_Jokes.to_numpy(), ratings_jokes, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = features_Jokes.to_numpy()\n",
    "Y_train = ratings_jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: R2 Score = 0.17253003213865992\n",
      "Fold 2: R2 Score = 0.25543560999061543\n",
      "Fold 3: R2 Score = 0.4707202334547642\n",
      "Average R2 Score: 0.2995619585280132\n"
     ]
    }
   ],
   "source": [
    "regressor = AdaBoostRegressor(random_state=42,n_estimators=20)\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "r2_scores = cross_val_score(regressor, X_train, Y_train, cv=kf, scoring='r2')\n",
    "\n",
    "for i, score in enumerate(r2_scores, 1):\n",
    "    print(f'Fold {i}: R2 Score = {score}')\n",
    "\n",
    "average_r2 = np.mean(r2_scores)\n",
    "print(f'Average R2 Score: {average_r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: R2 Score = 0.3523324121256475\n",
      "Fold 2: R2 Score = 0.24880533943538685\n",
      "Fold 3: R2 Score = 0.43750078616873\n",
      "Average R2 Score: 0.34621284590992146\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "regressor = SVR()\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "r2_scores = cross_val_score(regressor, X_train, Y_train, cv=kf, scoring='r2')\n",
    "\n",
    "for i, score in enumerate(r2_scores, 1):\n",
    "    print(f'Fold {i}: R2 Score = {score}')\n",
    "\n",
    "average_r2 = np.mean(r2_scores)\n",
    "print(f'Average R2 Score: {average_r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression - Fold 1: R2 Score = 0.28570630734033986\n",
      "Ridge Regression - Fold 2: R2 Score = 0.25435674361082017\n",
      "Ridge Regression - Fold 3: R2 Score = 0.28012271624086105\n",
      "Average R2 Score (Ridge Regression): 0.2733952557306737\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alpha = 10\n",
    "ridge_regressor = Ridge(alpha=alpha)\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "r2_scores_ridge = cross_val_score(ridge_regressor, X_train, Y_train, cv=kf, scoring='r2')\n",
    "\n",
    "for i, score in enumerate(r2_scores_ridge, 1):\n",
    "    print(f'Ridge Regression - Fold {i}: R2 Score = {score}')\n",
    "\n",
    "average_r2_ridge = np.mean(r2_scores_ridge)\n",
    "print(f'Average R2 Score (Ridge Regression): {average_r2_ridge}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Ridge Regression - Fold 1: R2 Score = 0.3409172080711239\n",
      "Bayesian Ridge Regression - Fold 2: R2 Score = 0.2824460394493197\n",
      "Bayesian Ridge Regression - Fold 3: R2 Score = 0.43033910865836966\n",
      "Average R2 Score (Bayesian Ridge Regression): 0.3512341187262711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "bayesian_regressor = BayesianRidge()\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "r2_scores_bayesian = cross_val_score(bayesian_regressor, X_train, Y_train, cv=kf, scoring='r2')\n",
    "\n",
    "for i, score in enumerate(r2_scores_bayesian, 1):\n",
    "    print(f'Bayesian Ridge Regression - Fold {i}: R2 Score = {score}')\n",
    "\n",
    "average_r2_bayesian = np.mean(r2_scores_bayesian)\n",
    "print(f'Average R2 Score (Bayesian Ridge Regression): {average_r2_bayesian}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR - Fold 1: R2 Score = 0.3597122787161956\n",
      "SVR - Fold 2: R2 Score = 0.29118920953727145\n",
      "SVR - Fold 3: R2 Score = 0.40012432453349733\n",
      "\n",
      "\n",
      "Testing result: 0.35034193759565485\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "C_value = 3\n",
    "svr_regressor = SVR(C=C_value)\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "models = []\n",
    "\n",
    "k_fold_score = []\n",
    "predictions = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X_train, Y_train), 1):\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "    Y_train_fold, Y_val_fold = Y_train[train_index], Y_train[val_index]\n",
    "\n",
    "    svr_regressor.fit(X_train_fold, Y_train_fold)\n",
    "    models.append(svr_regressor)\n",
    "    predictions_val = svr_regressor.predict(X_val_fold)\n",
    "\n",
    "    r2_score_fold = r2_score(y_true=Y_val_fold, y_pred=predictions_val)\n",
    "    k_fold_score.append(r2_score_fold)\n",
    "    \n",
    "\n",
    "\n",
    "    print(f'SVR - Fold {i}: R2 Score = {r2_score_fold}')\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Testing result: {np.mean(k_fold_score)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "# svr_Model = SVR(C=1)\n",
    "# svr_Model.fit(X_train, Y_train)\n",
    "# prediction = svr_Model.predict(X_test)\n",
    "# score = r2_score(Y_test,prediction)\n",
    "# print(f\"Final Score {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_r2(y_true, y_pred):\n",
    "#     mean_y_true = np.mean(y_true)\n",
    "    \n",
    "#     sst = np.sum((y_true - mean_y_true)**2)\n",
    "    \n",
    "#     ssr = np.sum((y_true - y_pred)**2)\n",
    "    \n",
    "#     r2 = 1 - (ssr / sst)\n",
    "    \n",
    "#     return r2\n",
    "\n",
    "# r2_score = calculate_r2(y_true=np.array(Y_test), y_pred=prediction)\n",
    "# print(f'R2 Score: {r2_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: /content/test_joke.txt\n"
     ]
    }
   ],
   "source": [
    "def read_jokes_from_file(file_path):\n",
    "    jokes = []\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                jokes.append(line.strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return jokes\n",
    "\n",
    "file_path = '/content/test_joke.txt'\n",
    "jokes_list = read_jokes_from_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_jokes(jokes, model):\n",
    "    preprocessed_test = preprocess_clean_jokes(jokes)\n",
    "    lemmitized_test = [lemmatize_text(joke) for joke in preprocessed_jokes]\n",
    "    extracted_test = feature_Extraction(preprocessed_test, lemmitized_test)\n",
    "    extracted_test = extracted_test.astype(int)\n",
    "    extracted_test.columns = [None] * len(extracted_test.columns)\n",
    "    extracted_test = extracted_test.to_numpy()\n",
    "    return model.predict(extracted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_Model = SVR(C=2)\n",
    "svr_Model.fit(X_train, Y_train)\n",
    "prediction = svr_Model.predict(X_test)\n",
    "predictions = testing_jokes(jokes_list,svr_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
